{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "by Tevfik Aytekin\n",
    "\n",
    "Matrix factorizarion is one of the state-of-the-art techniques used in recommender systems. Below you can find several different implementations.\n",
    "\n",
    "Over the years many variations of matrix factorization have been proposed. The following formulation is one of the simplest but which works well as we will see. It can be extended it various ways, see for example [Advances in Collaborative Filtering](https://link.springer.com/chapter/10.1007/978-1-4899-7637-6_3).\n",
    "\n",
    "Cost Function:\n",
    "$$\n",
    "J(\\Theta) =  \\sum_{u,i \\in K} (r_{ui} - q^T_ip_u)^2 + \\lambda(||q_i||^2+||p_u||^2)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $r_{ui}$ is the rating of user $u$ for item $i$.\n",
    "- $K$ is the set of $(u,i)$ pairs for which $r_{ui}$ is known.\n",
    "- $q_i$, $p_u$ are latent factor vectors for items and users, respectively. \n",
    "- $\\lambda$ is the regularization parameter.\n",
    "\n",
    "And the optimization objective:\n",
    "\n",
    "$$\n",
    "\\min_{p*,q*} \\sum_{u,i \\in K} (r_{ui} - q^T_ip_u)^2 + \\lambda(||q_i||^2+||p_u||^2)\n",
    "$$\n",
    "\n",
    "Typically the optimization done with gradient descent. To apply it we need to first find the partial derivative of the cost function with respect to latent variables which we will denote as $q_{fi}$ and $p_{fu}$. We can find the partial derivative as: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial p_{ku}}=-\\sum_{i \\in I_u}2(r_{ui} - q^T_ip_u)q_{ki} + 2\\lambda p_{ku}\n",
    "$$\n",
    "\n",
    "For **stochastic gradient descent** the update rule for the the $p_u$ vector for a single training example is:\n",
    "\n",
    "$$\n",
    "p_u = p_u + \\alpha ((r_{ui} - q^T_ip_u)q_{i} - \\lambda p_{u})\n",
    "$$\n",
    "\n",
    "similarly for $q_i$ vector we have:\n",
    "\n",
    "$$\n",
    "q_i = q_i + \\alpha ((r_{ui} - q^T_ip_u)p_{u} - \\lambda q_{u})\n",
    "$$\n",
    "\n",
    "For **batch gradient descent** the update rule for the the $p_u$ vector for all preferences where user $u$ appears:\n",
    "\n",
    "$$\n",
    "p_u = p_u + \\alpha (\\sum_{i \\in I_u}(r_{ui} - q^T_ip_u)q_{i} - \\lambda p_{u})\n",
    "$$\n",
    "\n",
    "similarly for $q_i$ vector we have:\n",
    "\n",
    "$$\n",
    "q_i = q_i + \\alpha (\\sum_{u \\in U_i} (r_{ui} - q^T_ip_u)p_{u} - \\lambda q_{u})\n",
    "$$\n",
    "\n",
    "In the above equations $I_u$ is the set of items rated by user $u$ and $U_i$ is the set of users who rated item $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import csr_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movielens dataset\n",
    "\n",
    "We will use the smallest Movielens 100k Dataset which includes 100k preferences. A preference is a triple (user, item, rating). You can download this data set from\n",
    "[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "Note the sparsity of the dataset which shows that most of the user/item matrix is empty. This is a typical property of the datasets in this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2          3\n",
       "0  196  242  3  881250949\n",
       "1  186  302  3  891717742\n",
       "2   22  377  1  878887116\n",
       "3  244   51  2  880606923\n",
       "4  166  346  1  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefs = pd.read_csv(\"../datasets/ml-100k/u.data\", sep=\"\\t\", header=None)\n",
    "prefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 943\n",
      "Number of items: 1682\n",
      "Number of preferences: 100000\n",
      "Sparsity: 0.06304669364224531\n"
     ]
    }
   ],
   "source": [
    "n_users = prefs.iloc[:,0].unique().size\n",
    "n_items = prefs.iloc[:,1].unique().size\n",
    "n_prefs = prefs.iloc[:,1].size\n",
    "n_factors = 10\n",
    "users = prefs.iloc[:,0].unique()\n",
    "items = prefs.iloc[:,1].unique()\n",
    "\n",
    "print(\"Number of users:\",n_users)\n",
    "print(\"Number of items:\",n_items)\n",
    "print(\"Number of preferences:\",n_prefs)\n",
    "print(\"Sparsity:\",n_prefs/(n_users*n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Function\n",
    "\n",
    "Error is calculated by predicting the rating of a user and an item in the test set using the factor representations of users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(X, u_factors, i_factors):\n",
    "    error = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        u_idx = X.iloc[i,0]\n",
    "        i_idx = X.iloc[i,1]\n",
    "        error += np.abs(X.iloc[i,2] - np.dot(u_factors[u_idx].T, i_factors[i_idx]))\n",
    "    return error/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Predictor Error\n",
    "\n",
    "The expected error of a random predictor would be 1.4 given that the actual ratings are uniformly distributed. Below is a function for calculating this error experimentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predictor_error(X):\n",
    "    error = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        u_idx = X.iloc[i,0]\n",
    "        i_idx = X.iloc[i,1]\n",
    "        error += np.abs(X.iloc[i,2] - np.random.randint(1,6))\n",
    "    return error/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize factor matrices\n",
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error:  [[3.52973149]]\n",
      "Random predictor error:  1.50763\n"
     ]
    }
   ],
   "source": [
    "# Find out the initial error (without learning) and the error of a random predictor\n",
    "print(\"Initial error: \", calc_error(prefs, user_factors,item_factors))\n",
    "print(\"Random predictor error: \", random_predictor_error(prefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Following is the stochastic gradient algorithm which is popularized by [Simon Funk](https://sifter.org/simon/journal/20061211.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "# Stochastic Gradient descent\n",
    "alpha = 0.03\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    X_train = shuffle(X_train)\n",
    "    for r in range(X_train.shape[0]):\n",
    "        u = X_train.iloc[r,0]\n",
    "        i = X_train.iloc[r,1]\n",
    "        error = X_train.iloc[r,2] - np.dot(user_factors[u].T, item_factors[i])[0,0]\n",
    "        user_factors[u] = user_factors[u] + alpha*(error*item_factors[i] - my_lambda*user_factors[u])\n",
    "        item_factors[i] = item_factors[i] + alpha*(error*user_factors[u] - my_lambda*item_factors[i])  \n",
    "       \n",
    "          \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train, user_factors, item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test, user_factors, item_factors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "If you run the code below you will see that both training and test errors decrease very slowly. Eventually there will be convergence but compared to stochastic version it will be very slow. It is a good example to show the speed advantage of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import copy\n",
    "#initialize factor matrices\n",
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for i in range(n_prefs):\n",
    "    user_factors[prefs.iloc[i,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[i,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "train_users = X_train.iloc[:,0].unique()\n",
    "train_items = X_train.iloc[:,1].unique()\n",
    "R = csr_matrix((X_train.iloc[:,2], (X_train.iloc[:,0],X_train.iloc[:,1])))\n",
    "\n",
    "# Batch Gradient descent\n",
    "alpha = 0.1\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    for u in train_users:\n",
    "        I_u = X_train[X_train[0]==u].iloc[:,1]\n",
    "        sum_total = 0\n",
    "        for i in I_u:\n",
    "            sum_total += (R[u,i] - np.dot(item_factors[i].T, user_factors[u])[0,0])*item_factors[i]\n",
    "        sum_total = sum_total / I_u.size\n",
    "        user_factors[u] = user_factors[u] + alpha*(sum_total - my_lambda*user_factors[u])\n",
    "    for i in train_items:\n",
    "        U_i = X_train[X_train[1]==i].iloc[:,0]\n",
    "        sum_total = 0\n",
    "        for u in U_i:\n",
    "            sum_total += (R[u,i] - np.dot(item_factors[i].T, user_factors[u])[0,0])*user_factors[u]\n",
    "        sum_total = sum_total / U_i.size\n",
    "        item_factors[i] = item_factors[i] + alpha*(sum_total - my_lambda*item_factors[i])\n",
    "        \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train,user_factors,item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test,user_factors,item_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares (ALS)\n",
    "Since the datasets in this domain are really large, people always try to find ways to speed up the optimization processes. One popular algorithm is called ALS. Here the basic idea is that although the cost function is not convex, when either user factors or items factors are fixed then it becomes a convex function which can be directly solved. The algorithm alternates between updating user and item factors. ([Large-scale parallel collaborative filtering for the netflix prize](https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32)). \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial p_{ku}}=-\\sum_{i \\in I_u}2(r_{ui} - q^T_ip_u)q_{ki} + 2\\lambda p_{ku} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I_u} q_{ki}q^T_ip_u + \\lambda p_{ku} = \\sum_{i \\in I_u}q_{ki}r_{ui} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I_u} q_{i}q^T_ip_u + \\lambda p_{u} = \\sum_{i \\in I_u}q_{i}r_{ui} \n",
    "$$\n",
    "\n",
    "$$\n",
    "(Q_{I_u}Q_{I_u}^T + \\lambda I) p_{u} = Q_{I_u}R^T(u,I_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{u} = (Q_{I_u}Q_{I_u}^T + \\lambda I)^{-1}Q_{I_u}R^T(u,I_u)\n",
    "$$\n",
    "\n",
    "where $I$ is the $f × f$ identity matrix. $Q_{I_u}$ denotes the sub-matrix of $Q$ where columns $j \\in I_u$ are selected, and $R^T(u,I_u)$ is the row vector where columns $j \\in I_u$ of the $u$-th row of $R$ are selected.\n",
    "\n",
    "Similarly for $q_i$ we have:\n",
    "\n",
    "$$\n",
    "q_{i} = (P_{U_i}P_{U_i}^T + \\lambda I)^{-1}P_{U_i}R(U_i,i)\n",
    "$$\n",
    "\n",
    "where $I$ is the $f × f$ identity matrix. $P_{U_i}$ denotes the sub-matrix of $P$ where columns $j \\in U_i$ are selected, and $R(U_i,i)$ is the column vector where columns $j \\in U_i$ of the $i$-th column of $R$ are selected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize factor matrices\n",
    "Q = pd.DataFrame(np.random.rand(n_factors,n_items)-0.5, columns=items)\n",
    "P = pd.DataFrame(np.random.rand(n_factors,n_users)-0.5, columns=users)\n",
    "\n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "train_users = X_train.iloc[:,0].unique()\n",
    "train_items = X_train.iloc[:,1].unique()\n",
    "R = csr_matrix((X_train.iloc[:,2], (X_train.iloc[:,0],X_train.iloc[:,1])))\n",
    "\n",
    "alpha = 0.030\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    for u in train_users:\n",
    "        I_u = X_train[X_train[0]==u].iloc[:,1]\n",
    "        A = np.dot(Q[I_u],Q[I_u].T)+my_lambda*np.identity(n_factors)\n",
    "        V = np.dot(Q[I_u],R[u,I_u].todense().T)\n",
    "        P[u] = np.dot(np.linalg.inv(A),V)\n",
    "    for i in train_items:\n",
    "        U_i = X_train[X_train[1]==i].iloc[:,0]\n",
    "        A = np.dot(P[U_i],P[U_i].T)+my_lambda*np.identity(n_factors)\n",
    "        V = np.dot(P[U_i],R[U_i,i].todense())     \n",
    "        Q[i] = np.dot(np.linalg.inv(A),V)\n",
    "        \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train,P,Q))\n",
    "    print(\"Test error: \", calc_error(X_test,P,Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

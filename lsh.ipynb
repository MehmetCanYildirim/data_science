{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Nearest Neighbor\n",
    "\n",
    "Finding nearest neighbors in a set of objects is a very general problem which has applications in many areas. If the size of the set of objects is very large then an exhaustive pairwise comparision of all ojects can be very costly.\n",
    "\n",
    "**Randomized near-neighbor reporting:** Given a set $P$ of points in a $d$-dimensional space $\\mathbb{R}^d$, and parameters $R > 0, > δ > 0$, construct a data structure which, given any query point $q$, reports each $R$-near neighbor of $q$ in $P$ with probability $1 − δ$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main idea of LSH\n",
    "\n",
    "The main idea of LSH is to design hash functions such that the probability of a collision is much higher for closer points compared to points which are far apart. Given such hash functions one can hash a query point and retrive the elements in the buckets that contain the query point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An LSH Function for Cosine Similarity\n",
    "\n",
    "<img src=\"images/lsh_cosine.jpg\" width = \"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measurement\n",
    "\n",
    "Nearest neighbor search: Suppose that we have a large dataset of vectors X and given a target vector we want to find the most similar k vectors to the target vector in the dataset X. Note that we can do this type of search more than once. \n",
    "\n",
    "First let us generate the dataset X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vectors = 100000\n",
    "dim = 10\n",
    "n_neighbors = 100\n",
    "n_queries = 2000\n",
    "n_random_vectors = 10\n",
    "\n",
    "target_vectors = np.random.randn(n_queries, dim)\n",
    "dataset = np.random.randn(n_vectors, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:19261.359930038452ms\n"
     ]
    }
   ],
   "source": [
    "ns = NaiveSearch(dataset)\n",
    "ns.build()\n",
    "tic = time.time()\n",
    "ns_nn = []\n",
    "for i in range(n_queries):\n",
    "    target_vector = target_vectors[i,:].reshape(1,dim)\n",
    "    neighbors = ns.find_nn(target_vector, n_neighbors)\n",
    "    ns_nn.append(neighbors)\n",
    "toc = time.time()\n",
    "print(\"Time:\"+str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:3326.2650966644287ms\n",
      "Hit ratio:  0.49389\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#target_vector_b = target_vector\n",
    "#print(neighbors)\n",
    "\n",
    "lsh_nn = []\n",
    "\n",
    "lsh = LSH(dataset)\n",
    "lsh.build(n_random_vectors, n_bands = 10)\n",
    "tic = time.time()\n",
    "for i in range(n_queries):\n",
    "    target_vector = target_vectors[i,:].reshape(1,dim)\n",
    "    neighbors = lsh.find_nn(target_vector, n_neighbors, n_bands = 10)\n",
    "    lsh_nn.append(neighbors)\n",
    "toc = time.time()\n",
    "print(\"Time:\"+str(1000*(toc-tic))+\"ms\")\n",
    "#target_vector_b = target_vector\n",
    "#print(neighbors)\n",
    "\n",
    "hits = [len(np.intersect1d(ns_nn[i], lsh_nn[i])) for i in range(n_queries)]\n",
    "#print(hits)\n",
    "print(\"Hit ratio: \",sum(hits) / (n_queries*n_neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsh.bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSearch:\n",
    "    def __init__(self, data):\n",
    "        # data is a n-by-d matrix where d is the length of the vectors\n",
    "        # and n is the number of vectors. \n",
    "        self.data = data\n",
    "        self.norms = None\n",
    "        self.data_normalized_T = None\n",
    "    \n",
    "        \n",
    "    def build(self):\n",
    "        self.norms = np.linalg.norm(self.data, axis=1)\n",
    "        self.norms.shape = (len(self.norms), 1)\n",
    "        \n",
    "        data_normalized = np.divide(self.data, self.norms)\n",
    "        self.data_normalized_T = data_normalized.T\n",
    "            \n",
    "            \n",
    "    def find_nn(self, target_vector, n_neighbors=10):\n",
    "\n",
    "        #target_vector_normalized = np.linalg.norm(target_vector)\n",
    "        \n",
    "        sims = np.dot(target_vector,self.data_normalized_T)[0]\n",
    "        return sims.argsort()[::-1][:n_neighbors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSH:\n",
    "    def __init__(self, data):\n",
    "        # data is a n-by-d matrix where d is the length of the vectors\n",
    "        # and n is the number of vectors. \n",
    "        self.data = data\n",
    "        self.bands = []\n",
    "        self.random_vectors = []\n",
    "    \n",
    "    def build(self, n_random_vectors, n_bands = 1):\n",
    "        for b in range(n_bands):\n",
    "            # generate random vectors\n",
    "            self.bands.append({})\n",
    "            dim = self.data.shape[1]\n",
    "            self.random_vectors.append(np.random.randn(n_random_vectors, dim))\n",
    "            # generate dim-by-n index bits\n",
    "            sign_bits = np.dot(self.data, self.random_vectors[b].T) >= 0\n",
    "            n_data_vectors = self.data.shape[0]\n",
    "            for i in range(n_data_vectors):\n",
    "                key = tuple(sign_bits[i,:])\n",
    "                if key not in self.bands[b]:\n",
    "                    self.bands[b][key] = []\n",
    "                self.bands[b][key].append(i)\n",
    "            \n",
    "            \n",
    "    def find_nn(self, target_vector, n_neighbors=10, n_bands = 1):\n",
    "\n",
    "        candidate_ids = []\n",
    "        for b in range(n_bands):\n",
    "            sign_bits = (np.dot(target_vector, self.random_vectors[b].T) >= 0).flatten()\n",
    "            sign_bits_tuple = tuple(sign_bits)\n",
    "            ids = self.bands[b].get(sign_bits_tuple)\n",
    "            if ids is None: \n",
    "                ids = []\n",
    "            candidate_ids = candidate_ids + ids\n",
    "        if len(candidate_ids) > 0:\n",
    "                candidate_vectors = self.data[candidate_ids, :]\n",
    "                sims = 1 - pairwise_distances(target_vector, candidate_vectors, metric='cosine').flatten()\n",
    "                sorted_nn = sims.argsort()[::-1][:n_neighbors]\n",
    "                return np.array([candidate_ids[i] for i in sorted_nn])\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the cost of pairwise similarity computation\n",
    "\n",
    "Suppose that we have n objects represented as vectors of size d. The cost of computing all pairwise similarities is $O(n^2d)$. When $n$ is large the cost of this computation can be quite large. And in some applications like finding near duplicate web pages, in order to eliminate them from search results, $n$ (number of web pages) can be really large.\n",
    "\n",
    "To get a sense of this cost below is a simple code which measures the time two multiply two matrices of size $n$-by-$d$. (Note that some version of multiplication is needed in order to find similarities between the vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "d = 1000\n",
    "X = np.random.randn(n,d)\n",
    "Y = np.random.randn(n,d)\n",
    "tic = time.process_time()\n",
    "\n",
    "z = np.dot(X,Y.T)\n",
    "\n",
    "toc = time.process_time()\n",
    "print(\"Time:\"+str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the time complexity is quadratic, we expect the figures in the following table if we increase $n$. You can try some larger $n$'s to test this.\n",
    "\n",
    "|  n | time  | \n",
    "|:---|:---|\n",
    "|  100k | 400 seconds  |\n",
    "|  1m | 11 hours  |\n",
    "|  10m | 46 days  |\n",
    "|  100m | 12 years  |\n",
    "|  1b | 12 centuries |\n",
    "\n",
    "The above results are taken on a 2,3 GHz Dual-Core Intel Core i5 laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1268.3916793505834"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40000000000/(60*60*24*365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(2,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "for i in combinations([1,2,3],1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for different_bits in combinations(range(num_vector), search_radius):\n",
    "            alternate_bits = copy(query_bin_bits)\n",
    "            for i in different_bits:\n",
    "                alternate_bits[i] = 1 if alternate_bits[i] == 0 else 0\n",
    "\n",
    "            # Convert the new bit vector to an integer index\n",
    "            nearby_bin = alternate_bits.dot(powers_of_two)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

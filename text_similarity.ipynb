{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Matrix\n",
    "table taken from [https://wordnetcode.princeton.edu/5papers.pdf](https://wordnetcode.princeton.edu/5papers.pdf)\n",
    "\n",
    "<img src=\"images/lexical_matrix.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "The word meaning $M_1$ in above table can be represented by the set of word forms that can be used to express it: {F1, F2, . . . }. These sets are called synonym sets (or simply synsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet Hierarchy\n",
    "Below is a simplified illustration of the hierarchy of wordnet.\n",
    "\n",
    "<img src=\"images/wordnet_hierarchy.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word is a set of meanings\n",
    "wn.synsets('word') gives these meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synset is a set of synonyms (word forms). Each synset corresponds to a concept/meaning. The nodes in the WordNet hierarchy corresponds to synsets. A synset is identified with a 3-part name of the form: word.pos.nn. For example, 'car.n.01' means the first meaning of 'car' used as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('car.n.01').definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas correspond to word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypernyms anf hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonymy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Similarity\n",
    "path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.16666666666666666\n",
      "0.07692307692307693\n",
      "0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "print(right.path_similarity(minke))\n",
    "print(right.path_similarity(orca))\n",
    "print(right.path_similarity(tortoise))\n",
    "print(right.path_similarity(novel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wordnet_hierarchy.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "compact = wn.synset('compact.n.03')\n",
    "hatchback = wn.synset('hatchback.n.01')\n",
    "print(motorcar.path_similarity(compact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(hatchback.path_similarity(compact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(hatchback.path_similarity(hatchback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ways for finding synonyms\n",
    "\n",
    "WordNet is constructed manually by experts of linguistics. There is also the computational approach to semantics. Below we will look at one such approach for finding synonyms. The approach relies on the below fundamental hypothesis:\n",
    "<br><br>\n",
    "<center><b>Distributional Hypothesis: similar words appear in similar contexts.</b></center>\n",
    "<br><br>\n",
    "We will first need to build a corpus and a co-occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(text):\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [] \n",
    "    for i in sent_tokenize(text): \n",
    "        for j in tokenizer.tokenize(i):\n",
    "            words.append(j.lower())\n",
    "    words = np.unique(words)\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words = [porter.stem(t) for t in words]\n",
    "    words = np.unique(words)\n",
    "    \n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    counter = 0;\n",
    "    for w in words:\n",
    "        word_to_index[w] = counter\n",
    "        index_to_word[counter] = w\n",
    "        counter += 1  \n",
    "    return words, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is data mining course cmp3004. It is about data mining. I like it so much.\"\n",
    "words, word_to_index, index_to_word = build_corpus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about' 'cmp3004' 'cours' 'data' 'i' 'is' 'it' 'like' 'mine' 'much' 'so'\n",
      " 'thi']\n",
      "{'about': 0, 'cmp3004': 1, 'cours': 2, 'data': 3, 'i': 4, 'is': 5, 'it': 6, 'like': 7, 'mine': 8, 'much': 9, 'so': 10, 'thi': 11}\n"
     ]
    }
   ],
   "source": [
    "corpus_size = len(words)\n",
    "print(words)\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['000', '10', '1816', '23rd', '24th', '26th', '28th', '7th', '8th',\n",
       "       '_', '_______', '_a_', '_accepted_', '_adair_', '_addition_',\n",
       "       '_all_', '_almost_', '_alone_', '_amor_', '_and_', '_answer_',\n",
       "       '_any_', '_appropriation_', '_as_', '_assistance_', '_at_',\n",
       "       '_bath_', '_be_', '_been_', '_blunder_', '_boiled_', '_both_',\n",
       "       '_bride_', '_broke_', '_caro_', '_cause_', '_chaperon_',\n",
       "       '_compassion_', '_compliments_', '_court_', '_courtship_', '_did_',\n",
       "       '_dissolved_', '_dixon_', '_dixons_', '_doubts_', '_each_',\n",
       "       '_eighteen_', '_elton_', '_engagement_', '_evening_', '_felt_',\n",
       "       '_first_', '_gentleman_', '_great_', '_greater_', '_had_',\n",
       "       '_half_', '_happily_', '_has_', '_have_', '_he_', '_her_',\n",
       "       '_here_', '_him_', '_his_', '_home_', '_housebreaking_', '_i_',\n",
       "       '_introduction_', '_invite_', '_is_', '_it_', '_joint_', '_just_',\n",
       "       '_lady_', '_letting_', '_little_', '_man_', '_married_', '_marry_',\n",
       "       '_may_', '_me_', '_mediocre_', '_misery_', '_miss_', '_moment_',\n",
       "       '_more_', '_most_', '_mr', '_mr_', '_mrs_', '_must_', '_my_',\n",
       "       '_named_', '_names_', '_nearer_', '_not_', '_now_', '_of_',\n",
       "       '_one_', '_our_', '_own_', '_part_', '_particular_', '_party_',\n",
       "       '_patriae_', '_perfection_', '_philip_', '_precious_', '_present_',\n",
       "       '_presume_', '_promise_', '_purport_', '_recollecting_',\n",
       "       '_refused_', '_repentance_', '_respect_', '_rev', '_robin_',\n",
       "       '_sacrifice_', '_say_', '_secret_', '_sensation_', '_shall_',\n",
       "       '_she_', '_ship_', '_should_', '_small_', '_some_', '_source_',\n",
       "       '_sposo_', '_taylor_', '_tell_', '_ten_', '_that_', '_the_',\n",
       "       '_them_', '_then_', '_there_', '_they_', '_thing_', '_think_',\n",
       "       '_thoughts_', '_three_', '_time_', '_times_', '_to_', '_told_',\n",
       "       '_treasures_', '_try_', '_two_', '_understand_', '_unreasonable_',\n",
       "       '_unrequited_', '_us_', '_very_', '_wanted_', '_was_', '_way_',\n",
       "       '_we_', '_well_', '_were_', '_what_', '_when_', '_white', '_who_',\n",
       "       '_will_', '_wish_', '_with_', '_woman_', '_woodhouse_', '_world_',\n",
       "       '_would_', '_you_', '_your_', 'a', 'abbey', 'abbot', 'abdi',\n",
       "       'abhor', 'abid', 'abil', 'abl', 'abod', 'abolit', 'abomin',\n",
       "       'about', 'abov', 'abroad', 'abrupt', 'abruptli', 'absenc',\n",
       "       'absent', 'absolut', 'absorb', 'abstain', 'absurd', 'abund',\n",
       "       'abundantli', 'abus', 'acced', 'accent', 'accept', 'access',\n",
       "       'accid', 'accident', 'accommod', 'accompani', 'accomplish',\n",
       "       'accord', 'accordingli', 'accost', 'account', 'accru', 'accumul',\n",
       "       'accus', 'accustom', 'ach', 'achiev', 'acknowledg', 'acquaint',\n",
       "       'acquiesc', 'acquir', 'acquit', 'acquitt', 'acr', 'across',\n",
       "       'acrost', 'act', 'action', 'activ', 'actual', 'acut', 'ad',\n",
       "       'adapt', 'add', 'addit', 'address', 'adelaid', 'adequ', 'adher',\n",
       "       'adieu', 'adjoin', 'administ', 'admir', 'admiss', 'admit', 'adopt',\n",
       "       'ador', 'advanc', 'advantag', 'adventur', 'adversari', 'advertis',\n",
       "       'advic', 'advis', 'advoc', 'affabl', 'affair', 'affect',\n",
       "       'affectedli', 'affection', 'affirm', 'affix', 'afflict', 'afford',\n",
       "       'affront', 'afloat', 'afraid', 'after', 'afternoon', 'afterward',\n",
       "       'again', 'against', 'age', 'aggrandis', 'aggress', 'aghast',\n",
       "       'agit', 'ago', 'agoni', 'agre', 'agreeabl', 'agreement',\n",
       "       'agricultur', 'ah', 'ahead', 'aid', 'aim', 'aimabl', 'air', 'airi',\n",
       "       'ajar', 'al', 'ala', 'alacr', 'aladdin', 'alarm', 'alderney',\n",
       "       'alert', 'alik', 'aliv', 'all', 'allay', 'allevi', 'alli',\n",
       "       'allianc', 'allow', 'alloy', 'allud', 'allus', 'alman', 'almost',\n",
       "       'alon', 'along', 'aloud', 'alphabet', 'alreadi', 'also', 'altar',\n",
       "       'alter', 'altern', 'although', 'altogeth', 'alway', 'am', 'amaz',\n",
       "       'amazingli', 'ambit', 'amend', 'amiabl', 'amid', 'amidst', 'amiss',\n",
       "       'amiti', 'among', 'amongst', 'amount', 'ampl', 'ampli', 'amus',\n",
       "       'an', 'analog', 'ancient', 'and', 'anecdot', 'angel', 'anger',\n",
       "       'angl', 'angri', 'ani', 'anim', 'ann', 'anna', 'announc', 'annoy',\n",
       "       'anoth', 'answer', 'antagonist', 'anticip', 'antidot', 'anxieti',\n",
       "       'anxiou', 'anxious', 'anyth', 'anywher', 'apart', 'apiec',\n",
       "       'apolog', 'apologis', 'apothecari', 'appar', 'apparatu', 'appeal',\n",
       "       'appear', 'appeas', 'appel', 'appendag', 'appetit', 'appl',\n",
       "       'appli', 'applic', 'appoint', 'appreci', 'apprehend', 'apprehens',\n",
       "       'approach', 'approb', 'appropri', 'approv', 'april', 'apt', 'arch',\n",
       "       'archli', 'ardent', 'are', 'argu', 'argument', 'aright', 'aris',\n",
       "       'arisen', 'arm', 'aros', 'around', 'arrang', 'arrear', 'arriv',\n",
       "       'arrog', 'arrow', 'arrowroot', 'art', 'arthur', 'articl',\n",
       "       'articul', 'artific', 'artifici', 'artist', 'artless', 'artlessli',\n",
       "       'as', 'ascend', 'ascertain', 'asham', 'asid', 'ask', 'asleep',\n",
       "       'asparagu', 'aspect', 'asper', 'aspers', 'aspir', 'assail',\n",
       "       'assembl', 'assent', 'assert', 'assist', 'associ', 'assort',\n",
       "       'assum', 'assur', 'astley', 'astonish', 'astonishingli', 'astray',\n",
       "       'asund', 'at', 'ate', 'atmospher', 'aton', 'attach', 'attack',\n",
       "       'attain', 'attempt', 'attend', 'attent', 'attest', 'attitud',\n",
       "       'attorney', 'attract', 'attribut', 'audibl', 'augment', 'augur',\n",
       "       'august', 'augusta', 'aunt', 'auspic', 'austen', 'author',\n",
       "       'authoris', 'autumn', 'avail', 'avenu', 'avert', 'avoid', 'avow',\n",
       "       'avowedli', 'awak', 'awaken', 'awar', 'away', 'awe', 'awkward',\n",
       "       'awok', 'aye', 'babi', 'back', 'backgammon', 'background',\n",
       "       'backward', 'bad', 'badli', 'bailiff', 'bait', 'bake', 'baker',\n",
       "       'balanc', 'bali', 'ball', 'ballroom', 'band', 'bandi', 'bang',\n",
       "       'banish', 'bank', 'bar', 'bare', 'bargain', 'barn', 'baronn',\n",
       "       'barouch', 'base', 'bash', 'basi', 'basin', 'basket', 'bate',\n",
       "       'bates', 'bath', 'battl', 'be', 'beam', 'bear', 'bearer', 'beat',\n",
       "       'beau', 'beaufet', 'beauti', 'beaver', 'becam', 'becaus', 'becom',\n",
       "       'bed', 'been', 'beer', 'beet', 'befor', 'beforehand', 'befriend',\n",
       "       'beg', 'began', 'begin', 'begun', 'behalf', 'behav', 'behaviour',\n",
       "       'beheld', 'behind', 'behindhand', 'behold', 'belief', 'believ',\n",
       "       'bell', 'bella', 'belong', 'belov', 'below', 'bench', 'bend',\n",
       "       'beneath', 'benefit', 'benevol', 'bent', 'besid', 'best', 'bestow',\n",
       "       'betray', 'better', 'between', 'bewild', 'bewitch', 'beyond',\n",
       "       'bia', 'bickerton', 'bid', 'biliou', 'bird', 'birmingham', 'birth',\n",
       "       'birthday', 'biscuit', 'bit', 'bitter', 'bitterli', 'black',\n",
       "       'blame', 'blameabl', 'blameless', 'blanch', 'blank', 'bleak',\n",
       "       'blend', 'bless', 'blind', 'blinder', 'block', 'blockhead',\n",
       "       'blood', 'bloom', 'blossom', 'blot', 'blow', 'blown', 'blue',\n",
       "       'blunder', 'blunt', 'blush', 'board', 'boarder', 'boast', 'bodi',\n",
       "       'boil', 'bond', 'bone', 'bonnet', 'book', 'boot', 'border', 'bore',\n",
       "       'born', 'borrow', 'bosom', 'both', 'bottom', 'bought', 'bound',\n",
       "       'bounti', 'bow', 'box', 'boy', 'boyish', 'brace', 'bragg', 'brain',\n",
       "       'braithwait', 'branch', 'bravado', 'bread', 'break', 'breakfast',\n",
       "       'breast', 'breath', 'bred', 'breed', 'brew', 'brick', 'bride',\n",
       "       'brief', 'briefli', 'bright', 'brighten', 'brighter', 'brightest',\n",
       "       'brillianc', 'brilliant', 'bring', 'brisk', 'briskli', 'bristol',\n",
       "       'broad', 'broader', 'broadli', 'broadway', 'broadwood', 'broil',\n",
       "       'broke', 'broken', 'broth', 'brother', 'brotherli', 'brought',\n",
       "       'brown', 'brunswick', 'brunt', 'brutal', 'build', 'built', 'bulki',\n",
       "       'buri', 'burn', 'burst', 'bushel', 'busi', 'busiest', 'bustl',\n",
       "       'but', 'butcher', 'butler', 'butter', 'button', 'buy', 'by', 'bye',\n",
       "       'c', 'cabbag', 'cabinet', 'cake', 'calcul', 'call', 'calm',\n",
       "       'calmer', 'calmli', 'came', 'cameo', 'camp', 'campbel', 'can',\n",
       "       'candid', 'candidli', 'candl', 'candlelight', 'candour', 'cannot',\n",
       "       'canvass', 'cap', 'capabl', 'capac', 'capit', 'capric', 'caprici',\n",
       "       'captain', 'captiou', 'captiv', 'car', 'card', 'care', 'career',\n",
       "       'careless', 'carelessli', 'caress', 'caro', 'carolin', 'carpet',\n",
       "       'carri', 'carriag', 'carrot', 'cart', 'case', 'casement', 'cast',\n",
       "       'catch', 'catherin', 'cattl', 'caught', 'caus', 'caution',\n",
       "       'cautiou', 'cautious', 'cavil', 'ceas', 'ceaseless', 'ceil',\n",
       "       'celibaci', 'celleri', 'censur', 'centr', 'ceremoni', 'certain',\n",
       "       'certainli', 'certainti', 'certifi', 'chain', 'chair', 'chais',\n",
       "       'chamber', 'chanc', 'chang', 'changeabl', 'channel', 'chapter',\n",
       "       'charact', 'characterist', 'charad', 'charg', 'charit', 'chariti',\n",
       "       'charm', 'charmingli', 'chat', 'chatter', 'chatti', 'cheap',\n",
       "       'check', 'checker', 'cheek', 'cheer', 'cheerful', 'cheerless',\n",
       "       'chees', 'cherish', 'cherri', 'chicken', 'chief', 'chiefli',\n",
       "       'chilblain', 'child', 'childhood', 'children', 'chili', 'chimney',\n",
       "       'choic', 'choos', 'chose', 'chosen', 'christen', 'christian',\n",
       "       'christma', 'church', 'churchil', 'churchwarden', 'chuse',\n",
       "       'cipher', 'circl', 'circul', 'circular', 'circumspect', 'circumst',\n",
       "       'citizen', 'civil', 'civilli', 'claim', 'clamor', 'clara', 'class',\n",
       "       'clayton', 'clean', 'clear', 'clearer', 'clearest', 'clearli',\n",
       "       'clemenc', 'clergyman', 'clerk', 'clever', 'cleverest', 'clifton',\n",
       "       'climat', 'cling', 'cloak', 'clock', 'close', 'closer', 'closest',\n",
       "       'closet', 'cloth', 'cloud', 'clover', 'clownish', 'club', 'clung',\n",
       "       'coach', 'coachman', 'coachmen', 'coars', 'coarser', 'coast',\n",
       "       'cobham', 'cockad', 'coddl', 'coffe', 'cogit', 'cold', 'colder',\n",
       "       'coldest', 'coldli', 'cole', 'collat', 'collect', 'collectedli',\n",
       "       'colonel', 'colour', 'column', 'combat', 'combin', 'come',\n",
       "       'comfort', 'comfortless', 'command', 'commandingli', 'commend',\n",
       "       'comment', 'commiser', 'commiss', 'commit', 'common', 'commonest',\n",
       "       'commonli', 'commonplac', 'commun', 'compani', 'companion',\n",
       "       'compar', 'comparison', 'compass', 'compassion', 'compat',\n",
       "       'compet', 'complac', 'complain', 'complaint', 'complais',\n",
       "       'complet', 'completest', 'complexion', 'compli', 'complianc',\n",
       "       'compliment', 'complimentari', 'compos', 'composedli', 'composit',\n",
       "       'composur', 'comprehend', 'comprehens', 'compress', 'compris',\n",
       "       'compunct', 'comtess', 'conceal', 'conceit', 'conceiv', 'concentr',\n",
       "       'concept', 'concern', 'concert', 'concess', 'concili', 'concis',\n",
       "       'conclud', 'conclus', 'concurr', 'condemn', 'condescend',\n",
       "       'condescens', 'condit', 'condol', 'conduc', 'conduct', 'confeder',\n",
       "       'confer', 'confess', 'confid', 'confidant', 'confidenti', 'confin',\n",
       "       'confirm', 'confound', 'confus', 'congratul', 'congratulatori',\n",
       "       'conjectur', 'conjug', 'connect', 'connexion', 'conniv', 'conquer',\n",
       "       'conquest', 'conscienc', 'consciou', 'conscious', 'consent',\n",
       "       'consequ', 'conservatori', 'consid', 'consider', 'consist',\n",
       "       'consol', 'constanc', 'constant', 'constantli', 'constern',\n",
       "       'constitut', 'constrain', 'construct', 'consult', 'consumpt',\n",
       "       'contain', 'contempl', 'contempt', 'contemptu', 'contend',\n",
       "       'content', 'conting', 'continu', 'contract', 'contradict',\n",
       "       'contrari', 'contrast', 'contribut', 'contrit', 'contriv',\n",
       "       'controul', 'conundrum', 'conveni', 'convers', 'convey', 'convict',\n",
       "       'convinc', 'convivi'], dtype='<U15')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = gutenberg.raw('austen-emma.txt')\n",
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "corpus_size = len(words)\n",
    "print(corpus_size)\n",
    "words[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix2(text, words, word_to_index, window=1):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    corpus_size = len(words)\n",
    "    co_matrix = np.zeros((corpus_size,corpus_size),dtype=int)\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(porter.stem(w.lower()))\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                co_matrix[word_to_index[w],word_to_index[sent[j]]] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix(text, window=1):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    counter = 0\n",
    "    co_matrix = pd.DataFrame();\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(w.lower())\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                if w == sent[j]:# skip the word itself\n",
    "                    co_matrix.loc[w,sent[j]] = 0\n",
    "                elif (w in co_matrix.index and sent[j] in co_matrix.columns) and not np.isnan(co_matrix.loc[w,sent[j]]):\n",
    "                    co_matrix.loc[w,sent[j]] += 1\n",
    "                else:\n",
    "                    co_matrix.loc[w,sent[j]] = 1\n",
    "    co_matrix.fillna(0, inplace=True)\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How tokenization with regex works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "data\n",
      "mining\n",
      "course\n",
      "cmp3004\n",
      "It\n",
      "is\n",
      "about\n",
      "data\n",
      "mining\n",
      "I\n",
      "like\n",
      "it\n",
      "so\n",
      "much\n"
     ]
    }
   ],
   "source": [
    "text = \"This is data mining course cmp3004. It is about data mining. I like it so much.\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for w in tokenizer.tokenize(text):  \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>data</th>\n",
       "      <th>mining</th>\n",
       "      <th>course</th>\n",
       "      <th>cmp3004</th>\n",
       "      <th>it</th>\n",
       "      <th>about</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "      <th>so</th>\n",
       "      <th>much</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mining</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmp3004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         this   is  data  mining  course  cmp3004   it  about    i  like   so  \\\n",
       "this      0.0  1.0   1.0     0.0     0.0      0.0  0.0    0.0  0.0   0.0  0.0   \n",
       "is        1.0  0.0   2.0     1.0     0.0      0.0  1.0    1.0  0.0   0.0  0.0   \n",
       "data      1.0  2.0   0.0     2.0     1.0      0.0  0.0    1.0  0.0   0.0  0.0   \n",
       "mining    0.0  1.0   2.0     0.0     1.0      1.0  0.0    1.0  0.0   0.0  0.0   \n",
       "course    0.0  0.0   1.0     1.0     0.0      1.0  0.0    0.0  0.0   0.0  0.0   \n",
       "cmp3004   0.0  0.0   0.0     1.0     1.0      0.0  0.0    0.0  0.0   0.0  0.0   \n",
       "it        0.0  1.0   0.0     0.0     0.0      0.0  0.0    1.0  1.0   1.0  1.0   \n",
       "about     0.0  1.0   1.0     1.0     0.0      0.0  1.0    0.0  0.0   0.0  0.0   \n",
       "i         0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   1.0  0.0   \n",
       "like      0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  1.0   0.0  1.0   \n",
       "so        0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   1.0  0.0   \n",
       "much      0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   0.0  1.0   \n",
       "\n",
       "         much  \n",
       "this      0.0  \n",
       "is        0.0  \n",
       "data      0.0  \n",
       "mining    0.0  \n",
       "course    0.0  \n",
       "cmp3004   0.0  \n",
       "it        1.0  \n",
       "about     0.0  \n",
       "i         0.0  \n",
       "like      0.0  \n",
       "so        1.0  \n",
       "much      0.0  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = build_co_matrix(text, 2)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 1, 2, 0, 2, 1, 0, 2, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 2, 0, 2, 1, 0, 2, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 2, 0, 2, 1, 0, 2, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "matrix = build_co_matrix2(text, words, word_to_index, 5)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4643, 4643)\n",
      "[[ 2  2  0 ...  0  0  0]\n",
      " [ 2  2  0 ...  0  0  0]\n",
      " [ 0  0  1 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 15  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "text = gutenberg.raw('austen-emma.txt')\n",
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "co_matrix = build_co_matrix2(text, words, word_to_index, 5)\n",
    "print(co_matrix.shape)\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity:\n",
    "Intuition: Dot product increases if both pairs have the same sign and decreases if pairs have different signs (similar to correlation, actually Pearson correlation is just cosine similarity of the mean centered vectors). Division by the norms is necessary to penalize vectors which has large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds cosine similarity between two vectors a and b\n",
    "def cosine(a, b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    return dot / (norma * normb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most similar words to the target word using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = word_to_index['brother']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4643,)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[target,:]\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4643, 1)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = np.reshape(word_vector,(word_vector.size,1 ))\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([112, 117,   7, ..., 838, 257,  34])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = np.dot(word_vector.T,co_matrix)\n",
    "sims = sims[0,:]\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 345, 4117, 4064, 2862, 2046,  176, 4453,  521, 2186, 2356])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.47722558,  5.47722558,  3.74165739, ..., 24.93992783,\n",
       "        9.38083152,  2.82842712])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = np.linalg.norm(co_matrix, axis=0)\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6947.785978281138"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sims = np.divide(sims,norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 669, 3716,  345, 2059,  356, 2717, 1262, 2046, 3585, 4117])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brother'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[669]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sister'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[3716]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 417, 3727, 2744, 4064, 2901, 1569, 1423, 2862, 1339, 3648])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[word_to_index['artist'],:]\n",
    "word_vector = np.reshape(word_vector,(1,word_vector.size))\n",
    "sims = np.dot(word_vector,co_matrix)\n",
    "sims = sims[0,:]\n",
    "norm_sims = np.divide(sims,norms)\n",
    "norm_sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skill'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[3727]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, brown\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Matrix\n",
    "table taken from [https://wordnetcode.princeton.edu/5papers.pdf](https://wordnetcode.princeton.edu/5papers.pdf)\n",
    "\n",
    "<img src=\"images/lexical_matrix.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "The word meaning $M_1$ in above table can be represented by the set of word forms that can be used to express it: {F1, F2, . . . }. These sets are called synonym sets (or simply synsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet Hierarchy\n",
    "Below is a simplified illustration of the hierarchy of wordnet.\n",
    "\n",
    "<img src=\"images/wordnet_hierarchy.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word is a set of meanings\n",
    "wn.synsets('word') gives these meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synset is a set of synonyms (word forms). Each synset corresponds to a concept/meaning. The nodes in the WordNet hierarchy corresponds to synsets. A synset is identified with a 3-part name of the form: word.pos.nn. For example, 'car.n.01' means the first meaning of 'car' used as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('car.n.01').definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas correspond to word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypernyms anf hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonymy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Similarity\n",
    "path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.16666666666666666\n",
      "0.07692307692307693\n",
      "0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "print(right.path_similarity(minke))\n",
    "print(right.path_similarity(orca))\n",
    "print(right.path_similarity(tortoise))\n",
    "print(right.path_similarity(novel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wordnet_hierarchy.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "compact = wn.synset('compact.n.03')\n",
    "hatchback = wn.synset('hatchback.n.01')\n",
    "print(motorcar.path_similarity(compact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(hatchback.path_similarity(compact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(hatchback.path_similarity(hatchback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ways for finding synonyms\n",
    "\n",
    "WordNet is constructed manually by experts of linguistics. There is also the computational approach to semantics. Below we will look at one such approach for finding synonyms. The approach relies on the below fundamental hypothesis:\n",
    "<br><br>\n",
    "<center><b>Distributional Hypothesis: similar words appear in similar contexts.</b></center>\n",
    "<br><br>\n",
    "We will first need to build a corpus and a co-occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(text):\n",
    "    \"\"\" \n",
    "  \n",
    "    Parameters: \n",
    "    text (string): A (long) string \n",
    "  \n",
    "    Returns: \n",
    "    words: A list of unique word names.\n",
    "    word_to_index: a mapping from word names to integers.\n",
    "    index_to_word: a mapping from integers to word names.\n",
    "  \n",
    "    \"\"\"\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [] \n",
    "    for i in sent_tokenize(text): \n",
    "        for j in tokenizer.tokenize(i):\n",
    "            words.append(j.lower())\n",
    "    words = np.unique(words)\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words = [porter.stem(t) for t in words]\n",
    "    words = np.unique(words)\n",
    "    \n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    counter = 0;\n",
    "    for w in words:\n",
    "        word_to_index[w] = counter\n",
    "        index_to_word[counter] = w\n",
    "        counter += 1  \n",
    "    return words, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is data mining course cmp3004. It is about data mining. I like it so much.\"\n",
    "words, word_to_index, index_to_word = build_corpus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about' 'cmp3004' 'cours' 'data' 'i' 'is' 'it' 'like' 'mine' 'much' 'so'\n",
      " 'thi']\n",
      "{'about': 0, 'cmp3004': 1, 'cours': 2, 'data': 3, 'i': 4, 'is': 5, 'it': 6, 'like': 7, 'mine': 8, 'much': 9, 'so': 10, 'thi': 11}\n"
     ]
    }
   ],
   "source": [
    "corpus_size = len(words)\n",
    "print(words)\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27114\n",
      "number of tokens:  2084675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '00', '000', '001', '002', '005', '01', '014', '018', '02',\n",
       "       '025', '027', '03', '035', '039', '04', '040', '043', '046', '05',\n",
       "       '054', '06', '060', '0600', '062', '065', '07', '075', '076', '08',\n",
       "       '080', '081', '082', '083', '09', '090', '0c', '0f', '1', '10',\n",
       "       '100', '1000', '1001', '101', '101b', '102', '1020', '1024', '103',\n",
       "       '104', '1040', '1040a', '1044', '105', '106', '1065', '1066',\n",
       "       '1068', '107', '108', '109', '10th', '11', '110', '1105', '111',\n",
       "       '113', '114', '115', '1159', '116', '1162', '117', '1184', '119',\n",
       "       '11th', '12', '120', '1200', '121', '1213', '1215', '122', '1223',\n",
       "       '123', '1231', '124', '125', '125th', '126', '127', '128', '129',\n",
       "       '1290', '1298', '12a', '12th', '13', '130', '1300', '1307', '1310',\n",
       "       '1311', '132', '133', '135', '137', '138', '139', '13th', '14',\n",
       "       '140', '1409', '141', '1416', '142', '143', '144', '145', '1450',\n",
       "       '1453', '147', '1479', '148', '149', '1492', '14th', '15', '150',\n",
       "       '1500', '151', '1514', '1515', '1516', '153', '154', '1540',\n",
       "       '1543', '155', '1550', '1558', '156', '1565', '157', '1577',\n",
       "       '1579', '158', '1581', '1582', '1589', '1590', '1592', '1593',\n",
       "       '1594', '1595', '1596', '1597', '1598', '1599', '15th', '16',\n",
       "       '160', '1600', '1601', '1602', '1605', '1607', '1608', '1609',\n",
       "       '161', '1610', '1611', '1613', '1615', '162', '1622', '1624',\n",
       "       '1625', '1626', '1628', '1629', '1630', '1631', '1632', '1633',\n",
       "       '1637', '1638', '1639', '164', '1640', '1642', '1643', '1644',\n",
       "       '165', '1655', '1657', '166', '1665', '1667', '1671', '1674',\n",
       "       '1678', '168', '1680', '1687', '1688', '169', '1690', '1692',\n",
       "       '1693', '16th', '17', '170', '1700', '1702', '1707', '1709', '171',\n",
       "       '1714', '172', '1720', '1721', '1724', '1727', '1728', '172nd',\n",
       "       '173', '1730', '1731', '1732', '1733', '174', '1745', '1746',\n",
       "       '1747', '1748', '175', '1750', '1751', '1755', '1764', '1769',\n",
       "       '177', '1770', '1771', '1773', '1774', '1776', '1777', '1778',\n",
       "       '1780', '1781', '1782', '1783', '1785', '1786', '1787', '1788',\n",
       "       '1789', '179', '1791', '1792', '1793', '1797', '1799', '17e',\n",
       "       '17th', '18', '180', '1800', '1801', '1802', '1803', '1804',\n",
       "       '1805', '1806', '1807', '1808', '1810', '1811', '1812', '1813',\n",
       "       '1814', '1815', '1816', '1817', '1818', '1819', '182', '1820',\n",
       "       '1821', '1822', '1823', '1825', '1826', '1827', '1830', '1831',\n",
       "       '1832', '1833', '1834', '1835', '1837', '1838', '1839', '184',\n",
       "       '1840', '1841', '1842', '1843', '1844', '1845', '1846', '1847',\n",
       "       '1848', '1849', '185', '1850', '1851', '1852', '1853', '1854',\n",
       "       '1855', '1857', '1858', '1859', '185th', '186', '1860', '1861',\n",
       "       '1862', '1863', '1864', '1865', '1866', '1867', '1868', '1869',\n",
       "       '187', '1870', '1871', '1872', '1873', '1874', '1875', '1876',\n",
       "       '1878', '1879', '188', '1880', '1881', '1882', '1883', '1884',\n",
       "       '1885', '1886', '1887', '1888', '1889', '189', '1890', '1891',\n",
       "       '1892', '1893', '1895', '1896', '1897', '1898', '1899', '18e',\n",
       "       '18th', '19', '190', '1900', '1901', '1902', '1903', '1904',\n",
       "       '1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912',\n",
       "       '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192',\n",
       "       '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927',\n",
       "       '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935',\n",
       "       '1936', '1937', '1938', '1939', '1940', '1941', '1942', '1943',\n",
       "       '1944', '1945', '1946', '1947', '1948', '1949', '195', '1950',\n",
       "       '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1957b',\n",
       "       '1958', '1959', '196', '1960', '1961', '1962', '1963', '1964',\n",
       "       '1965', '1966', '1967', '1970', '1971', '1972', '1976', '198',\n",
       "       '1980', '1981', '1984', '1986', '1991', '19th', '1a', '1m', '2',\n",
       "       '20', '200', '2000', '202', '203', '2048', '205', '2051', '206',\n",
       "       '207', '208', '20th', '21', '210', '2100', '211', '2118', '212',\n",
       "       '213', '2130', '214', '215', '216', '218', '21st', '22', '220',\n",
       "       '221', '22111', '222', '2230', '224', '225', '2269', '227', '2274',\n",
       "       '228', '229', '22nd', '23', '230', '235', '236', '237', '239',\n",
       "       '23a', '23d', '23rd', '24', '240', '2400', '242', '2433', '2454',\n",
       "       '247', '24th', '25', '250', '2500', '251', '253', '254', '2544',\n",
       "       '255', '256', '257', '258', '25th', '26', '260', '261', '265',\n",
       "       '266', '268', '2688', '269', '26th', '27', '270', '2705', '271',\n",
       "       '272', '273', '2731', '275', '276', '278', '27th', '28', '280',\n",
       "       '2809', '281', '2825', '283', '285', '286', '28th', '29', '293',\n",
       "       '294', '295', '297', '2991', '29th', '2a', '2d', '2nd', '3', '30',\n",
       "       '300', '3000', '300th', '302', '306', '307', '30th', '31', '310',\n",
       "       '3100', '312', '313', '314', '31730', '3181', '31978', '31st',\n",
       "       '32', '320', '320tr', '3211', '323', '3247', '325', '327', '328',\n",
       "       '33', '330', '3300', '332', '334', '337', '338', '33d', '34',\n",
       "       '340', '340tr', '34220', '343', '344', '346', '348', '35', '350',\n",
       "       '3500', '3505o', '350th', '353', '354', '355', '357', '36', '360',\n",
       "       '361', '362', '3646', '365', '366', '367', '368', '369', '36th',\n",
       "       '37', '372', '375', '376', '379', '37th', '38', '380', '381',\n",
       "       '385', '387', '389', '39', '390', '391', '392', '394', '395',\n",
       "       '399', '3m', '3mm', '3rd', '4', '40', '400', '4000', '401', '402',\n",
       "       '405', '407', '41', '410', '412', '413', '414', '415', '417',\n",
       "       '418', '419', '42', '420', '4200', '425', '427', '42d', '43',\n",
       "       '430', '431', '434', '44', '44001', '44002', '44005', '44006',\n",
       "       '44007', '441', '442', '443', '444', '447', '45', '450', '451',\n",
       "       '452', '457', '46', '460', '461', '462', '463', '469', '46th',\n",
       "       '47', '470', '48', '480', '484', '4865771', '488', '489', '49',\n",
       "       '490', '4911', '492', '495', '499', '49er', '49th', '4th', '5',\n",
       "       '50', '500', '5000', '503', '5031', '505', '508', '509', '50th',\n",
       "       '51', '510', '511', '512', '514', '514c', '5155', '517', '51st',\n",
       "       '52', '520', '524', '525', '526', '52h', '52nd', '53', '532',\n",
       "       '538', '54', '540', '541', '542', '543', '545', '54th', '55',\n",
       "       '550', '553', '555', '557', '56', '560', '561', '5612', '562',\n",
       "       '565', '566', '56a', '57', '570', '571', '573', '5777', '58',\n",
       "       '580', '581', '5835', '5847', '585', '586', '589', '58th', '59',\n",
       "       '590', '5th', '6', '60', '600', '6000', '601', '602', '603', '604',\n",
       "       '605', '606', '607', '608', '609', '61', '610', '6124', '613',\n",
       "       '615', '616', '617', '619', '61st', '62', '622', '625', '63',\n",
       "       '634', '635', '637', '638', '639', '63d', '64', '642', '643',\n",
       "       '645', '646', '65', '650', '66', '666', '66th', '67', '671', '675',\n",
       "       '676', '677', '679', '68', '687', '689', '69', '6934', '694',\n",
       "       '695', '6a', '6th', '7', '70', '700', '701', '701st', '7026',\n",
       "       '7034', '704', '707', '7070', '7074', '70th', '71', '710', '72',\n",
       "       '720', '725', '7287', '72nd', '73', '734', '738', '74', '740',\n",
       "       '741', '742', '742c', '744', '748', '75', '750', '753', '754',\n",
       "       '758', '7599', '75th', '76', '760', '762', '764', '767', '768',\n",
       "       '77', '770', '78', '780', '78th', '79', '790', '792', '795', '798',\n",
       "       '7a', '7th', '8', '80', '800', '807', '80th', '81', '816', '817',\n",
       "       '82', '821', '823', '825', '827', '828', '83', '833', '836',\n",
       "       '83rd', '84', '840', '841', '842', '85', '850', '86', '865', '867',\n",
       "       '869', '87', '870', '871', '877', '87th', '88', '883', '885',\n",
       "       '8861', '887', '89', '892', '899', '8th', '9', '90', '900', '901',\n",
       "       '906', '91', '910', '918', '92', '920', '923', '9230', '926', '93',\n",
       "       '9329', '938', '94', '940i', '944', '949', '95', '950', '954',\n",
       "       '96', '960', '961', '963', '97', '98', '987', '989', '99', '991',\n",
       "       '9a', '9b', '9e', '9n', '9th', 'a', 'a135', 'a40', 'a5', 'aa',\n",
       "       'aaa', 'aaawww', 'aab', 'aah', 'aaron', 'ab', 'ab1', 'ab4',\n",
       "       'ab63711', 'aback', 'abandon', 'abaring', 'abas', 'abat',\n",
       "       'abatuno', 'abb', 'abba', 'abber', 'abbey'], dtype='<U20')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = brown.raw()\n",
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "corpus_size = len(words)\n",
    "print(corpus_size)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"number of tokens: \", len(tokens))\n",
    "words[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix2(text, words, word_to_index, window=1):\n",
    "    \"\"\" \n",
    "    Build a co-occurrence matrix \n",
    "    \n",
    "    Parameters: \n",
    "    text (string): A long string to be split into sentences.\n",
    "    words: A list of unique word names.\n",
    "    word_to_index: a mapping from word names to integers.\n",
    "    window: The size of the context window.\n",
    "  \n",
    "    Returns: \n",
    "    co_matrix: ndarray \n",
    "  \n",
    "    \"\"\"\n",
    "    porter = nltk.PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    corpus_size = len(words)\n",
    "    co_matrix = np.zeros((corpus_size,corpus_size),dtype=int)\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(porter.stem(w.lower()))\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                co_matrix[word_to_index[w],word_to_index[sent[j]]] += 1\n",
    "        np.fill_diagonal(co_matrix,0)\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_matrix(text, window=1):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    counter = 0\n",
    "    co_matrix = pd.DataFrame();\n",
    "    for s in sent_tokenize(text): \n",
    "        sent = [] \n",
    "        for w in tokenizer.tokenize(s):        \n",
    "            sent.append(w.lower())\n",
    "        for i, w in enumerate(sent):\n",
    "            for j in range(max(i-window,0),min(i+window+1,len(sent))):\n",
    "                if w == sent[j]:# skip the word itself\n",
    "                    co_matrix.loc[w,sent[j]] = 0\n",
    "                elif (w in co_matrix.index and sent[j] in co_matrix.columns) and not np.isnan(co_matrix.loc[w,sent[j]]):\n",
    "                    co_matrix.loc[w,sent[j]] += 1\n",
    "                else:\n",
    "                    co_matrix.loc[w,sent[j]] = 1\n",
    "    co_matrix.fillna(0, inplace=True)\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How tokenization with regex works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "data\n",
      "mining\n",
      "course\n",
      "cmp3004\n",
      "It\n",
      "is\n",
      "about\n",
      "data\n",
      "mining\n",
      "I\n",
      "like\n",
      "it\n",
      "so\n",
      "much\n"
     ]
    }
   ],
   "source": [
    "text = \"This is data mining course cmp3004. It is about data mining. I like it so much.\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for w in tokenizer.tokenize(text):  \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>data</th>\n",
       "      <th>mining</th>\n",
       "      <th>course</th>\n",
       "      <th>cmp3004</th>\n",
       "      <th>it</th>\n",
       "      <th>about</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "      <th>so</th>\n",
       "      <th>much</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mining</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmp3004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         this   is  data  mining  course  cmp3004   it  about    i  like   so  \\\n",
       "this      0.0  1.0   1.0     0.0     0.0      0.0  0.0    0.0  0.0   0.0  0.0   \n",
       "is        1.0  0.0   2.0     1.0     0.0      0.0  1.0    1.0  0.0   0.0  0.0   \n",
       "data      1.0  2.0   0.0     2.0     1.0      0.0  0.0    1.0  0.0   0.0  0.0   \n",
       "mining    0.0  1.0   2.0     0.0     1.0      1.0  0.0    1.0  0.0   0.0  0.0   \n",
       "course    0.0  0.0   1.0     1.0     0.0      1.0  0.0    0.0  0.0   0.0  0.0   \n",
       "cmp3004   0.0  0.0   0.0     1.0     1.0      0.0  0.0    0.0  0.0   0.0  0.0   \n",
       "it        0.0  1.0   0.0     0.0     0.0      0.0  0.0    1.0  1.0   1.0  1.0   \n",
       "about     0.0  1.0   1.0     1.0     0.0      0.0  1.0    0.0  0.0   0.0  0.0   \n",
       "i         0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   1.0  0.0   \n",
       "like      0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  1.0   0.0  1.0   \n",
       "so        0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   1.0  0.0   \n",
       "much      0.0  0.0   0.0     0.0     0.0      0.0  1.0    0.0  0.0   0.0  1.0   \n",
       "\n",
       "         much  \n",
       "this      0.0  \n",
       "is        0.0  \n",
       "data      0.0  \n",
       "mining    0.0  \n",
       "course    0.0  \n",
       "cmp3004   0.0  \n",
       "it        1.0  \n",
       "about     0.0  \n",
       "i         0.0  \n",
       "like      0.0  \n",
       "so        1.0  \n",
       "much      0.0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = build_co_matrix(text, 2)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 2, 0, 0, 1, 0, 2, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "matrix = build_co_matrix2(text, words, word_to_index, 5)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27114, 27114)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text = brown.raw()\n",
    "words, word_to_index, index_to_word = build_corpus(text)\n",
    "co_matrix = build_co_matrix2(text, words, word_to_index, 5)\n",
    "print(co_matrix.shape)\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity:\n",
    "Intuition: Dot product increases if both pairs have the same sign and decreases if pairs have different signs (similar to correlation, actually Pearson correlation is just cosine similarity of the mean centered vectors). Division by the norms is necessary to penalize vectors which has large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   1,   1,   1,   1,   1,   1,   1,   2,  29,   2,   1,   2,\n",
       "         3,   1,   2,  11,   2,   2,   1,   7,   1,   2,   1,   8,   1,\n",
       "         1,  17,   2,   1,   3,   1,   1,   1,   1,   1,   1,   6,   4,\n",
       "         1,   1,   1,   5,   7,   1,   9,   1,   1,   1,   1,   2,  10,\n",
       "         1,   3,   1,   2,   1,   1,  32,   1,  10,   1,   3,   3,   1,\n",
       "        10,   2,   1,   1,   1,   1,   3,   1,   1,   1,  21, 127,   1,\n",
       "         1,   1,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,   2,\n",
       "         1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   2,   1,   1,   1,   3,   1,   8,   1,   1,   3,   1,\n",
       "         1,   1,   1,   4,   1,   1,   1, 106,   1,   1,   1,   2,  10,\n",
       "        15,   1,   3,   1,   1,   1,   1,   3,   4,   1,   1,   7,   1,\n",
       "         4,   1,   3,   1,   4,   1,   1,   2,   1,   1,  14, 137,   1,\n",
       "         2,   1,   5,   1,  31,   3,   1,   9,   2,   1,   1,   1,   5,\n",
       "         1,   1,   1,   1,   4,   1,   1,   1,   4,   1,   1,   1,  14,\n",
       "         2,   1,   1,   2,   2,   2,   1,   1,   1,   1,   1,   1,   2,\n",
       "         1,   1,   1,   2,   1,   3,   2,  13,   2,   3,   1,   1,   1,\n",
       "         1,  34,   1,   2,   1,   1,   1,   1,   2,   6,   3,   6,  11,\n",
       "         3,   1,   1,   1,   1,   1,   1,   1,   4,   1,   2,   1,   1,\n",
       "         1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_matrix[:,0][np.nonzero(co_matrix[:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds cosine similarity between two vectors a and b\n",
    "def cosine(a, b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    return dot / (norma * normb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most similar words to the target word using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3621"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = word_to_index['book']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27114,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[target,:]\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27114, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = np.reshape(word_vector,(word_vector.size,1 ))\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117120,  31834, 476572, ...,   3606,    938,   2328])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = np.dot(word_vector.T,co_matrix)\n",
    "sims = sims[0,:]\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12229, 16505,  2343, 23958, 13061, 16875,  4753, 24294,  1783,\n",
       "       24296])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[12229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([231.17742104,  61.57921727, 809.71599959, ...,   7.07106781,\n",
       "         4.24264069,   6.        ])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = np.linalg.norm(co_matrix, axis=0)\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.409673645990857"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sims = np.divide(sims,norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3621,  8957, 13715, 19713,  5609, 26775, 16887, 14957, 17524,\n",
       "       14157])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sims.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[3621]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'famili'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[8957]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3621,  8957, 13715, 19713,  5609, 26775, 16887, 14957, 17524,\n",
       "       14157])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[word_to_index['book'],:]\n",
    "word_vector = np.reshape(word_vector,(1,word_vector.size))\n",
    "sims = np.dot(word_vector,co_matrix)\n",
    "sims = sims[0,:]\n",
    "norm_sims = np.divide(sims,norms)\n",
    "top10 = norm_sims.argsort()[::-1][:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "famili\n",
      "land\n",
      "record\n",
      "command\n",
      "word\n",
      "offic\n",
      "mass\n",
      "paper\n",
      "line\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top10)):\n",
    "    print(index_to_word[top10[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9818,  9022, 15911, 17579, 15530, 22291,  9110, 22466, 22349,\n",
       "       26037])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = co_matrix[word_to_index['friend'],:]\n",
    "word_vector = np.reshape(word_vector,(1,word_vector.size))\n",
    "sims = np.dot(word_vector,co_matrix)\n",
    "sims = sims[0,:]\n",
    "norm_sims = np.divide(sims,norms)\n",
    "top10 = norm_sims.argsort()[::-1][:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend\n",
      "father\n",
      "mother\n",
      "parent\n",
      "mine\n",
      "son\n",
      "fellow\n",
      "speech\n",
      "soul\n",
      "voic\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top10)):\n",
    "    print(index_to_word[top10[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

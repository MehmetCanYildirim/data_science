{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8nmvzS1wal2"
   },
   "source": [
    "# Word2Vec from Scratch\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtGrCtTzwal3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import gutenberg, brown\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from queue import PriorityQueue\n",
    "\n",
    "\n",
    "# You need to call nltk.download() to download all the nltk corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Lok_93Ewal6"
   },
   "source": [
    "## Definition from Wikipedia:\n",
    "\n",
    "“Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "slHGBPk9wal6",
    "outputId": "201100d6-fa43-4572-bdf0-5ffb7595c1f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/tevfikaytekin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 57340\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "num_sents = len(brown.sents())\n",
    "print(\"number of sentences:\", num_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UNiOp5Xwal-"
   },
   "source": [
    "An example sentence represented as a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "hTeu-Agwwal_",
    "outputId": "21cafd26-d787-4237-8609-faf1eb800c2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is an example application of word2vec using Gensim library. You can see some of the parameters and can find all the details of Gensim implementation [here](https://radimrehurek.com/gensim/models/word2vec.html). The Gensim word2vec source code is [here](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py) and the original source code by Mikolov can be found [here](https://github.com/tmikolov/word2vec/blob/master/word2vec.c).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "ShLkVpzUwamF",
    "outputId": "e1dc9dcb-d178-43d8-8160-063126edbfe9"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(brown.sents(),min_count = 5,\n",
    "                              size = 30, window = 5, iter=5, negative=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCLJktzTwamI"
   },
   "source": [
    "An example vector representation of the word \"book\". Since we set size = 30, the representation is an array of 30 reals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "hzmfOyduwamJ",
    "outputId": "2fe7d901-63e3-49db-bb37-7d882e3104f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04130429,  0.4521888 ,  0.27449906,  0.35119957,  0.26082107,\n",
       "       -0.3592744 ,  0.35024637, -0.0669838 , -0.9695614 ,  0.79387116,\n",
       "        0.01254979, -0.17283162, -0.9944789 , -0.26214194, -0.28051236,\n",
       "       -0.65149343, -0.11017204,  0.07075944,  0.24606021, -0.03542815,\n",
       "        0.1115644 ,  0.53932977, -0.59932303,  0.3196837 ,  0.45755264,\n",
       "       -0.46898037,  0.15049635,  0.51160663, -0.54451406,  0.05359796],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to test the performance of word2vec is to look at most similar words to a given word. Below you will find most similar words of the words \"book\" and \"eight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "lf2Nt7qhwamL",
    "outputId": "5bab91d5-2ed6-4e70-e17c-a3a03e6f815f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('story', 0.8666895031929016),\n",
       " ('novel', 0.8523391485214233),\n",
       " ('opinion', 0.8166705369949341),\n",
       " ('statement', 0.8038854002952576),\n",
       " ('era', 0.797407865524292),\n",
       " ('letter', 0.7884780168533325),\n",
       " ('poem', 0.7792933583259583),\n",
       " ('dream', 0.7791569232940674),\n",
       " ('Lord', 0.7782471179962158),\n",
       " ('title', 0.7781429886817932)]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "fAeXOOtpwamN",
    "outputId": "2126b29e-c02a-4a74-f898-c9ba852cc8ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seven', 0.946410596370697),\n",
       " ('six', 0.9121511578559875),\n",
       " ('thirty', 0.9009974002838135),\n",
       " ('five', 0.8982510566711426),\n",
       " ('eleven', 0.8856104016304016),\n",
       " ('four', 0.8812878131866455),\n",
       " ('ten', 0.8807625770568848),\n",
       " ('several', 0.8781701922416687),\n",
       " ('months', 0.8745277523994446),\n",
       " ('previous', 0.8717256784439087)]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='eight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the results are quite amazing. But it might not be so for every word, for example for the word \"angry\" the results are not very satisying. However, if have used a larger text the results could be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "Nw3uZuNdxAxO",
    "outputId": "2d6acd8a-7942-4f2b-d860-6dbccc218417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('waiter', 0.90479576587677),\n",
       " ('grief', 0.9026034474372864),\n",
       " ('servant', 0.9021402597427368),\n",
       " ('anger', 0.900694727897644),\n",
       " ('occasional', 0.8982645869255066),\n",
       " ('painful', 0.8944272994995117),\n",
       " ('impartial', 0.8934457898139954),\n",
       " ('mourning', 0.8911497592926025),\n",
       " ('joke', 0.8905177116394043),\n",
       " ('pious', 0.8897886276245117)]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='angry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlOxTodOwamP"
   },
   "source": [
    "You can also find (cosine) similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6nNBVelwamQ",
    "outputId": "75c3a85c-ac2b-448f-ad53-0c99cfb53fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'book' and 'story': 0.9399007\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'book' and 'story':\", \n",
    "    model.wv.similarity('book', 'story')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'book' and 'eight': 0.5157527\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'book' and 'eight':\", \n",
    "    model.wv.similarity('book', 'eight')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEs8zFMOwamS"
   },
   "source": [
    "## word2vec from scratch\n",
    "\n",
    "Now we will write word2vec from scratch. Note that the purpose of this implementation is to help understand the theory behind word2vec. The implementation is not meant to be efficient so the running time is quite slow compared to the Gensim implementation. However, the code is simpler and shows the main ingredients of word2vec.\n",
    "\n",
    "Different objectives can be used for word2vec. The following is the objective for word2vec with negative sampling. The main idea behind this objective is to find paramater values which maximizes the dot product of word representations which are in the same context and minimizes the dot product of word representations which are not in the same context.\n",
    "\n",
    "Where do log and sigma come from??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKPGXjrHwamS"
   },
   "source": [
    "$$\n",
    "J(\\Theta) = \\underset{\\theta}{\\operatorname{argmax}}{\\sum_{c,t \\in D_p}log(\\sigma(v_c \\cdot v_t))+\\sum_{c,t \\in D_n}log(\\sigma(-v_c \\cdot v_t))}\n",
    "$$\n",
    "\n",
    "Here, $D_p$ is the set of word pairs whose distance is at most $m$ and $D_n$ is the set of unrelated (negative) word pairs, i.e., word pairs whose distance is larger than $m$, and $\\sigma$ is the sigmoid function. Below we find the derivative of this function with respect to positive and negative words which we will use in the updates of gradient descent algorithm.\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial v_{c}}=\\sum_{c,t \\in D_p}\\frac{1}{\\sigma(v_c \\cdot v_t)}\\sigma(v_c \\cdot v_t)(1-\\sigma(v_c \\cdot v_t))(v_t)\\\\\n",
    "+ \\sum_{c,t \\in D_n}\\frac{1}{\\sigma(-v_c \\cdot v_t)}\\sigma(-v_c \\cdot v_t)(1-\\sigma(-v_c \\cdot v_t))(-v_t)\\\\\n",
    "= \\sum_{c,t \\in D_p}(1-\\sigma(v_c \\cdot v_t))v_t + \\sum_{c,t \\in D_n}-(1-\\sigma(-v_c \\cdot v_t))v_t \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial v_{t \\in D_p}}=\\sum_{c,t \\in D_p}(1-\\sigma(v_c \\cdot v_t))v_c \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial v_{t \\in D_n}}=\\sum_{c,t \\in D_n}-(1-\\sigma(-v_c \\cdot v_t))v_c \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmjwFkqPwamT"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Efrh96rzwama"
   },
   "outputs": [],
   "source": [
    "def build_indices(sents):\n",
    "    \"\"\" \n",
    "  \n",
    "    Parameters: \n",
    "    sents: A list of sentecens and each sentence is a list of words (i.e., a list of lists). \n",
    "  \n",
    "    Returns: \n",
    "    word_freqs: frequency of each word\n",
    "    word_to_index: a mapping from word names to integers.\n",
    "    index_to_word: a mapping from integers to word names.\n",
    "    \n",
    "  \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    word_freqs = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    for i in range(len(sents)): \n",
    "        for j in range(len(sents[i])):\n",
    "            w = sents[i][j].lower()\n",
    "            if w in word_freqs:\n",
    "                word_freqs[w] += 1\n",
    "            else:\n",
    "                word_freqs[w] = 1\n",
    "                word_to_index[w] = counter\n",
    "                index_to_word[counter] = w\n",
    "                counter += 1\n",
    "            \n",
    "    return word_freqs, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICcBKeI2wame"
   },
   "outputs": [],
   "source": [
    "def build_training_set(sents, word_freqs, window=5, sampling_freq = 0.001, neg_exp = 0.75, num_negs = 1, min_count=5):\n",
    "    \"\"\" \n",
    "    Builds a trainig set\n",
    "    \n",
    "    Parameters: \n",
    "    sents: A list of sentecens and each sentence is a list of words (i.e., a list of lists).\n",
    "    word_freqs: Frequency of words.\n",
    "    windows: size of the context window.\n",
    "    sampling_freq: words whose frequency larger than this value will be discarded.\n",
    "    neg_exp: used for adjusting the negative sampling distribution.\n",
    "    window: The size of the context window.\n",
    "  \n",
    "    Returns: \n",
    "    training_set: list of context word, positive and negatives\n",
    "    \"\"\"\n",
    "    words_list = []\n",
    "    total_freq = sum(word_freqs.values())\n",
    "    \n",
    "    #total_freq = sum([freq**(neg_exp) for freq in word_freqs.values()])\n",
    "    word_array = []\n",
    "    for word, freq in word_freqs.items():\n",
    "        if ((word_freqs[word]/total_freq) < sampling_freq) and (word_freqs[word] > min_count):\n",
    "            words_list.append(word)\n",
    "            for i in range(int(freq**neg_exp)):\n",
    "                word_array.append(word)\n",
    "    \n",
    "    training_set = []\n",
    "    \n",
    "    sampled_sents = []\n",
    "    for i in range(len(sents)): \n",
    "        sent = []\n",
    "        for j in range(len(sents[i])):\n",
    "            w = sents[i][j].lower()\n",
    "            if ((word_freqs[w] / total_freq) < sampling_freq) and (word_freqs[w] > min_count):\n",
    "                sent.append(w)\n",
    "        sampled_sents.append(sent)\n",
    "    \n",
    "    \n",
    "    for i in range(len(sampled_sents)): \n",
    " \n",
    "        for j, w in enumerate(sampled_sents[i]):\n",
    "            context = []\n",
    "            for k in range(max(j-window,0),min(j+window+1,len(sampled_sents[i]))):\n",
    "                w_p = sampled_sents[i][k]\n",
    "                if (w == w_p):\n",
    "                    continue\n",
    "                w_n = []\n",
    "                for k in range(num_negs):\n",
    "                    w_n.append(word_array[np.random.randint(0,len(word_array))] )\n",
    "                training_set.append([w,w_p,w_n])\n",
    "\n",
    "    return training_set, np.unique(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the produced training_set here is a very simple example sentence consisting of 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ha5PR9xJwamg",
    "outputId": "2e87c77c-02b3-4a49-a844-826abad9b79e"
   },
   "outputs": [],
   "source": [
    "sents = [[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"]]\n",
    "word_freqs = {\"a\":1,\"b\":1,\"c\":1,\"d\":1,\"e\":1,\"f\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "XdzNepMvwamj",
    "outputId": "62979a2d-e445-4590-a6db-4de64b966a19"
   },
   "outputs": [],
   "source": [
    "training_set, words_list = build_training_set(sents,window=1, word_freqs= word_freqs , sampling_freq = 1, min_count= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fulton', 'county', ['rigidly']],\n",
       " ['fulton', 'grand', ['dr.']],\n",
       " ['fulton', 'jury', ['son']],\n",
       " ['fulton', 'friday', ['thing']],\n",
       " ['fulton', 'investigation', ['oils']],\n",
       " ['county', 'fulton', ['sorbed']],\n",
       " ['county', 'grand', ['corps']],\n",
       " ['county', 'jury', ['henrietta']],\n",
       " ['county', 'friday', ['pouring']],\n",
       " ['county', 'investigation', ['without']],\n",
       " ['county', 'recent', ['generous']],\n",
       " ['grand', 'fulton', ['facing']],\n",
       " ['grand', 'county', ['mary']],\n",
       " ['grand', 'jury', ['pat']],\n",
       " ['grand', 'friday', ['bright']],\n",
       " ['grand', 'investigation', ['away']],\n",
       " ['grand', 'recent', ['here']],\n",
       " ['grand', 'primary', ['city']],\n",
       " ['jury', 'fulton', ['vent']],\n",
       " ['jury', 'county', ['shape']],\n",
       " ['jury', 'grand', ['whether']],\n",
       " ['jury', 'friday', ['judgments']],\n",
       " ['jury', 'investigation', ['payment']],\n",
       " ['jury', 'recent', ['per']],\n",
       " ['jury', 'primary', ['period']],\n",
       " ['jury', 'election', ['revolution']],\n",
       " ['friday', 'fulton', ['wilson']],\n",
       " ['friday', 'county', ['information']],\n",
       " ['friday', 'grand', ['seemed']],\n",
       " ['friday', 'jury', ['again']],\n",
       " ['friday', 'investigation', ['rated']],\n",
       " ['friday', 'recent', ['ratio']],\n",
       " ['friday', 'primary', ['manifest']],\n",
       " ['friday', 'election', ['detected']],\n",
       " ['friday', 'produced', ['battery']],\n",
       " ['investigation', 'fulton', ['reasonably']],\n",
       " ['investigation', 'county', ['enough']],\n",
       " ['investigation', 'grand', ['bob']],\n",
       " ['investigation', 'jury', ['least']],\n",
       " ['investigation', 'friday', ['disagree']],\n",
       " ['investigation', 'recent', ['type']],\n",
       " ['investigation', 'primary', ['range']],\n",
       " ['investigation', 'election', ['moment']],\n",
       " ['investigation', 'produced', ['mostly']],\n",
       " ['investigation', 'evidence', ['wave']],\n",
       " ['recent', 'county', ['students']],\n",
       " ['recent', 'grand', ['police']],\n",
       " ['recent', 'jury', ['cent']],\n",
       " ['recent', 'friday', ['infinite']],\n",
       " ['recent', 'investigation', ['snapped']],\n",
       " ['recent', 'primary', ['hand']],\n",
       " ['recent', 'election', ['calif.']],\n",
       " ['recent', 'produced', ['aid']],\n",
       " ['recent', 'evidence', ['just']],\n",
       " ['recent', 'irregularities', ['many']],\n",
       " ['primary', 'grand', ['hall']],\n",
       " ['primary', 'jury', ['quality']],\n",
       " ['primary', 'friday', ['rural']],\n",
       " ['primary', 'investigation', ['fabrics']],\n",
       " ['primary', 'recent', ['blatz']],\n",
       " ['primary', 'election', ['beverly']],\n",
       " ['primary', 'produced', ['apparently']],\n",
       " ['primary', 'evidence', ['subsequent']],\n",
       " ['primary', 'irregularities', ['smaller']],\n",
       " ['primary', 'took', ['softened']],\n",
       " ['election', 'jury', ['snoring']],\n",
       " ['election', 'friday', ['follows']],\n",
       " ['election', 'investigation', ['nadine']],\n",
       " ['election', 'recent', ['units']],\n",
       " ['election', 'primary', ['gate']],\n",
       " ['election', 'produced', ['too']],\n",
       " ['election', 'evidence', ['drill']],\n",
       " ['election', 'irregularities', ['1']],\n",
       " ['election', 'took', ['spending']],\n",
       " ['election', 'place', ['cannot']],\n",
       " ['produced', 'friday', ['comparable']],\n",
       " ['produced', 'investigation', ['sharp']],\n",
       " ['produced', 'recent', ['s.']],\n",
       " ['produced', 'primary', ['black']],\n",
       " ['produced', 'election', ['paper']],\n",
       " ['produced', 'evidence', ['football']],\n",
       " ['produced', 'irregularities', ['miles']],\n",
       " ['produced', 'took', ['great']],\n",
       " ['produced', 'place', ['unified']],\n",
       " ['evidence', 'investigation', ['factor']],\n",
       " ['evidence', 'recent', ['down']],\n",
       " ['evidence', 'primary', ['across']],\n",
       " ['evidence', 'election', ['current']],\n",
       " ['evidence', 'produced', ['preach']],\n",
       " ['evidence', 'irregularities', ['younger']],\n",
       " ['evidence', 'took', ['given']],\n",
       " ['evidence', 'place', ['opium']],\n",
       " ['irregularities', 'recent', ['whole']],\n",
       " ['irregularities', 'primary', ['most']],\n",
       " ['irregularities', 'election', ['disposed']],\n",
       " ['irregularities', 'produced', ['whipple']],\n",
       " ['irregularities', 'evidence', ['independence']],\n",
       " ['irregularities', 'took', ['station']],\n",
       " ['irregularities', 'place', ['still']],\n",
       " ['took', 'primary', ['example']],\n",
       " ['took', 'election', ['disposition']],\n",
       " ['took', 'produced', ['put']],\n",
       " ['took', 'evidence', ['trying']],\n",
       " ['took', 'irregularities', ['u.']],\n",
       " ['took', 'place', ['buttons']],\n",
       " ['place', 'election', ['beethoven']],\n",
       " ['place', 'produced', ['dancers']],\n",
       " ['place', 'evidence', ['impetus']],\n",
       " ['place', 'irregularities', ['wine']],\n",
       " ['place', 'took', ['types']],\n",
       " ['jury', 'further', ['plan']],\n",
       " ['jury', 'city', ['filed']],\n",
       " ['jury', 'executive', ['senses']],\n",
       " ['jury', 'committee', ['touch']],\n",
       " ['jury', 'over-all', ['information']],\n",
       " ['further', 'jury', ['recall']],\n",
       " ['further', 'city', ['required']],\n",
       " ['further', 'executive', ['tsh']],\n",
       " ['further', 'committee', ['securing']],\n",
       " ['further', 'over-all', ['savior']],\n",
       " ['further', 'charge', ['engaged']],\n",
       " ['city', 'jury', ['illustrated']],\n",
       " ['city', 'further', ['special']],\n",
       " ['city', 'executive', ['species']],\n",
       " ['city', 'committee', ['cut']],\n",
       " ['city', 'over-all', ['quietly']],\n",
       " ['city', 'charge', ['license']],\n",
       " ['city', 'election', ['adequate']],\n",
       " ['executive', 'jury', ['japan']],\n",
       " ['executive', 'further', ['sum']],\n",
       " ['executive', 'city', ['existed']],\n",
       " ['executive', 'committee', ['trace']],\n",
       " ['executive', 'over-all', ['grey']],\n",
       " ['executive', 'charge', ['shadow']],\n",
       " ['executive', 'election', ['spreads']],\n",
       " ['executive', 'deserves', ['second']],\n",
       " ['committee', 'jury', ['minds']],\n",
       " ['committee', 'further', ['wheeled']],\n",
       " ['committee', 'city', ['costs']],\n",
       " ['committee', 'executive', ['careful']],\n",
       " ['committee', 'over-all', ['inspector']],\n",
       " ['committee', 'charge', ['methods']],\n",
       " ['committee', 'election', ['interest']],\n",
       " ['committee', 'deserves', ['episode']],\n",
       " ['committee', 'praise', ['continues']],\n",
       " ['over-all', 'jury', ['meaningless']],\n",
       " ['over-all', 'further', ['amusing']],\n",
       " ['over-all', 'city', ['stairs']],\n",
       " ['over-all', 'executive', ['quantity']],\n",
       " ['over-all', 'committee', ['matter']],\n",
       " ['over-all', 'charge', ['living']],\n",
       " ['over-all', 'election', ['interest']],\n",
       " ['over-all', 'deserves', ['runaway']],\n",
       " ['over-all', 'praise', ['commonplace']],\n",
       " ['over-all', 'thanks', [\"that's\"]],\n",
       " ['charge', 'further', ['lengths']],\n",
       " ['charge', 'city', ['smaller']],\n",
       " ['charge', 'executive', ['dropped']],\n",
       " ['charge', 'committee', ['byrd']],\n",
       " ['charge', 'over-all', ['wings']],\n",
       " ['charge', 'election', ['heavily']],\n",
       " ['charge', 'deserves', ['number']],\n",
       " ['charge', 'praise', ['amount']],\n",
       " ['charge', 'thanks', ['funds']],\n",
       " ['charge', 'city', ['loves']],\n",
       " ['election', 'city', ['after']],\n",
       " ['election', 'executive', ['three']],\n",
       " ['election', 'committee', ['frightening']],\n",
       " ['election', 'over-all', ['singing']],\n",
       " ['election', 'charge', ['sovereign']],\n",
       " ['election', 'deserves', ['snarled']],\n",
       " ['election', 'praise', ['involves']],\n",
       " ['election', 'thanks', ['horses']],\n",
       " ['election', 'city', ['three']],\n",
       " ['election', 'atlanta', ['true']],\n",
       " ['deserves', 'executive', ['way']],\n",
       " ['deserves', 'committee', ['pressure']],\n",
       " ['deserves', 'over-all', ['net']],\n",
       " ['deserves', 'charge', ['significant']],\n",
       " ['deserves', 'election', ['youth']],\n",
       " ['deserves', 'praise', ['ruins']],\n",
       " ['deserves', 'thanks', ['carried']],\n",
       " ['deserves', 'city', ['according']],\n",
       " ['deserves', 'atlanta', ['progress']],\n",
       " ['deserves', 'manner', ['paris']],\n",
       " ['praise', 'committee', ['st.']],\n",
       " ['praise', 'over-all', ['work']],\n",
       " ['praise', 'charge', ['writings']],\n",
       " ['praise', 'election', ['allow']],\n",
       " ['praise', 'deserves', ['winning']],\n",
       " ['praise', 'thanks', ['where']],\n",
       " ['praise', 'city', ['e.']],\n",
       " ['praise', 'atlanta', ['despite']],\n",
       " ['praise', 'manner', ['clark']],\n",
       " ['praise', 'election', ['kinds']],\n",
       " ['thanks', 'over-all', ['asleep']],\n",
       " ['thanks', 'charge', ['engines']],\n",
       " ['thanks', 'election', ['pistol']],\n",
       " ['thanks', 'deserves', ['university']],\n",
       " ['thanks', 'praise', ['greek']],\n",
       " ['thanks', 'city', ['fog']],\n",
       " ['thanks', 'atlanta', ['because']],\n",
       " ['thanks', 'manner', ['gore']],\n",
       " ['thanks', 'election', ['promised']],\n",
       " ['thanks', 'conducted', ['tremendous']],\n",
       " ['city', 'charge', ['children']],\n",
       " ['city', 'election', ['conformity']],\n",
       " ['city', 'deserves', ['happened']],\n",
       " ['city', 'praise', ['very']],\n",
       " ['city', 'thanks', ['customer']],\n",
       " ['city', 'atlanta', ['hotels']],\n",
       " ['city', 'manner', ['milwaukee']],\n",
       " ['city', 'election', ['herself']],\n",
       " ['city', 'conducted', ['contributed']],\n",
       " ['atlanta', 'election', ['town']],\n",
       " ['atlanta', 'deserves', ['much']],\n",
       " ['atlanta', 'praise', ['should']],\n",
       " ['atlanta', 'thanks', ['prime']],\n",
       " ['atlanta', 'city', ['kind']],\n",
       " ['atlanta', 'manner', ['men']],\n",
       " ['atlanta', 'election', ['profession']],\n",
       " ['atlanta', 'conducted', ['cases']],\n",
       " ['manner', 'deserves', ['course']],\n",
       " ['manner', 'praise', ['deck']],\n",
       " ['manner', 'thanks', ['get']],\n",
       " ['manner', 'city', ['too']],\n",
       " ['manner', 'atlanta', ['substance']],\n",
       " ['manner', 'election', ['number']],\n",
       " ['manner', 'conducted', ['ceremonies']],\n",
       " ['election', 'praise', ['riding']],\n",
       " ['election', 'thanks', ['known']],\n",
       " ['election', 'city', ['lean']],\n",
       " ['election', 'atlanta', ['poetic']],\n",
       " ['election', 'manner', ['discomfort']],\n",
       " ['election', 'conducted', ['church']],\n",
       " ['conducted', 'thanks', ['substituted']],\n",
       " ['conducted', 'city', ['took']],\n",
       " ['conducted', 'atlanta', ['5,000']],\n",
       " ['conducted', 'manner', ['magical']],\n",
       " ['conducted', 'election', ['great']],\n",
       " ['term', 'jury', ['discrimination']],\n",
       " ['term', 'charged', ['female']],\n",
       " ['term', 'fulton', ['cerebral']],\n",
       " ['term', 'superior', ['raise']],\n",
       " ['term', 'court', ['propaganda']],\n",
       " ['jury', 'term', ['slice']],\n",
       " ['jury', 'charged', ['programing']],\n",
       " ['jury', 'fulton', ['begin']],\n",
       " ['jury', 'superior', ['record']],\n",
       " ['jury', 'court', ['opened']],\n",
       " ['jury', 'judge', ['team']],\n",
       " ['charged', 'term', ['errors']],\n",
       " ['charged', 'jury', ['directions']],\n",
       " ['charged', 'fulton', ['abel']],\n",
       " ['charged', 'superior', ['differences']],\n",
       " ['charged', 'court', ['program']],\n",
       " ['charged', 'judge', ['making']],\n",
       " ['charged', 'investigate', ['partner']],\n",
       " ['fulton', 'term', ['years']],\n",
       " ['fulton', 'jury', ['chatter']],\n",
       " ['fulton', 'charged', ['brass']],\n",
       " ['fulton', 'superior', ['necessity']],\n",
       " ['fulton', 'court', ['unadjusted']],\n",
       " ['fulton', 'judge', ['formulated']],\n",
       " ['fulton', 'investigate', ['king']],\n",
       " ['fulton', 'reports', ['completely']],\n",
       " ['superior', 'term', ['leading']],\n",
       " ['superior', 'jury', ['diocs']],\n",
       " ['superior', 'charged', ['mystery']],\n",
       " ['superior', 'fulton', ['realizing']],\n",
       " ['superior', 'court', ['naval']],\n",
       " ['superior', 'judge', ['thrown']],\n",
       " ['superior', 'investigate', ['another']],\n",
       " ['superior', 'reports', ['1940']],\n",
       " ['superior', 'possible', ['loaded']],\n",
       " ['court', 'term', ['conventional']],\n",
       " ['court', 'jury', ['involving']],\n",
       " ['court', 'charged', ['proved']],\n",
       " ['court', 'fulton', ['cafe']],\n",
       " ['court', 'superior', ['dried']],\n",
       " ['court', 'judge', ['hope']],\n",
       " ['court', 'investigate', ['apparent']],\n",
       " ['court', 'reports', ['too']],\n",
       " ['court', 'possible', ['fresh']],\n",
       " ['court', 'irregularities', ['behavior']],\n",
       " ['judge', 'jury', ['clothes']],\n",
       " ['judge', 'charged', ['colleges']],\n",
       " ['judge', 'fulton', ['partners']],\n",
       " ['judge', 'superior', ['sounded']],\n",
       " ['judge', 'court', ['pope']],\n",
       " ['judge', 'investigate', ['house']],\n",
       " ['judge', 'reports', ['schedule']],\n",
       " ['judge', 'possible', ['shares']],\n",
       " ['judge', 'irregularities', ['1945']],\n",
       " ['judge', 'primary', ['certain']],\n",
       " ['investigate', 'charged', ['character']],\n",
       " ['investigate', 'fulton', ['temperatures']],\n",
       " ['investigate', 'superior', ['operating']],\n",
       " ['investigate', 'court', ['win']],\n",
       " ['investigate', 'judge', ['starts']],\n",
       " ['investigate', 'reports', [\"one's\"]],\n",
       " ['investigate', 'possible', ['train']],\n",
       " ['investigate', 'irregularities', ['seemed']],\n",
       " ['investigate', 'primary', ['become']],\n",
       " ['investigate', 'won', ['concluded']],\n",
       " ['reports', 'fulton', ['tommy']],\n",
       " ['reports', 'superior', ['february']],\n",
       " ['reports', 'court', ['demonstrating']],\n",
       " ['reports', 'judge', ['light']],\n",
       " ['reports', 'investigate', ['storage']],\n",
       " ['reports', 'possible', ['dust']],\n",
       " ['reports', 'irregularities', ['furnish']],\n",
       " ['reports', 'primary', ['death']],\n",
       " ['reports', 'won', ['worried']],\n",
       " ['reports', 'allen', ['services']],\n",
       " ['possible', 'superior', ['construction']],\n",
       " ['possible', 'court', ['trooper']],\n",
       " ['possible', 'judge', ['conditions']],\n",
       " ['possible', 'investigate', ['filthy']],\n",
       " ['possible', 'reports', ['countenance']],\n",
       " ['possible', 'irregularities', ['indeed']],\n",
       " ['possible', 'primary', ['february']],\n",
       " ['possible', 'won', ['son']],\n",
       " ['possible', 'allen', ['appearance']],\n",
       " ['possible', 'jr.', ['paces']],\n",
       " ['irregularities', 'court', ['before']],\n",
       " ['irregularities', 'judge', ['screen']],\n",
       " ['irregularities', 'investigate', ['effluent']],\n",
       " ['irregularities', 'reports', ['soil']],\n",
       " ['irregularities', 'possible', ['member']],\n",
       " ['irregularities', 'primary', ['rusty']],\n",
       " ['irregularities', 'won', ['internal']],\n",
       " ['irregularities', 'allen', ['phenomena']],\n",
       " ['irregularities', 'jr.', ['most']],\n",
       " ['primary', 'judge', ['logs']],\n",
       " ['primary', 'investigate', ['shouted']],\n",
       " ['primary', 'reports', ['skyros']],\n",
       " ['primary', 'possible', ['unto']],\n",
       " ['primary', 'irregularities', [\"they're\"]],\n",
       " ['primary', 'won', ['restrict']],\n",
       " ['primary', 'allen', ['impression']],\n",
       " ['primary', 'jr.', [\"papa's\"]],\n",
       " ['won', 'investigate', ['locked']],\n",
       " ['won', 'reports', ['fall']],\n",
       " ['won', 'possible', ['strongly']],\n",
       " ['won', 'irregularities', ['values']],\n",
       " ['won', 'primary', ['missed']],\n",
       " ['won', 'allen', ['efforts']],\n",
       " ['won', 'jr.', ['world']],\n",
       " ['allen', 'reports', ['lion']],\n",
       " ['allen', 'possible', ['shouting']],\n",
       " ['allen', 'irregularities', ['absent']],\n",
       " ['allen', 'primary', ['plenty']],\n",
       " ['allen', 'won', ['varied']],\n",
       " ['allen', 'jr.', ['racing']],\n",
       " ['jr.', 'possible', ['package']],\n",
       " ['jr.', 'irregularities', ['loudly']],\n",
       " ['jr.', 'primary', ['almost']],\n",
       " ['jr.', 'won', ['leadership']],\n",
       " ['jr.', 'allen', ['survivors']],\n",
       " ['relative', 'handful', ['applying']],\n",
       " ['relative', 'reports', ['spared']],\n",
       " ['relative', 'received', ['structure']],\n",
       " ['relative', 'jury', ['truman']],\n",
       " ['relative', 'considering', ['hiding']],\n",
       " ['handful', 'relative', ['africa']],\n",
       " ['handful', 'reports', ['match']],\n",
       " ['handful', 'received', ['wide']],\n",
       " ['handful', 'jury', ['tactical']],\n",
       " ['handful', 'considering', ['speech']],\n",
       " ['handful', 'widespread', ['wilderness']],\n",
       " ['reports', 'relative', ['chose']],\n",
       " ['reports', 'handful', ['parties']],\n",
       " ['reports', 'received', ['item']],\n",
       " ['reports', 'jury', [\"government's\"]],\n",
       " ['reports', 'considering', ['breathing']],\n",
       " ['reports', 'widespread', ['treated']],\n",
       " ['reports', 'interest', ['cut']],\n",
       " ['received', 'relative', ['reflection']],\n",
       " ['received', 'handful', ['developed']],\n",
       " ['received', 'reports', ['being']],\n",
       " ['received', 'jury', ['reduced']],\n",
       " ['received', 'considering', ['permitted']],\n",
       " ['received', 'widespread', ['zen']],\n",
       " ['received', 'interest', ['accreditation']],\n",
       " ['received', 'election', ['inherited']],\n",
       " ['jury', 'relative', ['money']],\n",
       " ['jury', 'handful', ['racing']],\n",
       " ['jury', 'reports', ['pounds']],\n",
       " ['jury', 'received', ['thus']],\n",
       " ['jury', 'considering', ['design']],\n",
       " ['jury', 'widespread', ['department']],\n",
       " ['jury', 'interest', ['startling']],\n",
       " ['jury', 'election', ['claim']],\n",
       " ['jury', 'number', ['along']],\n",
       " ['considering', 'relative', ['stuart']],\n",
       " ['considering', 'handful', ['sacred']],\n",
       " ['considering', 'reports', ['longer']],\n",
       " ['considering', 'received', ['serve']],\n",
       " ['considering', 'jury', ['one-half']],\n",
       " ['considering', 'widespread', ['1931']],\n",
       " ['considering', 'interest', ['ashore']],\n",
       " ['considering', 'election', ['40,000']],\n",
       " ['considering', 'number', ['sending']],\n",
       " ['considering', 'voters', ['task']],\n",
       " ['widespread', 'handful', ['never']],\n",
       " ['widespread', 'reports', ['bar']],\n",
       " ['widespread', 'received', ['india']],\n",
       " ['widespread', 'jury', ['measured']],\n",
       " ['widespread', 'considering', ['every']],\n",
       " ['widespread', 'interest', ['three']],\n",
       " ['widespread', 'election', ['near']],\n",
       " ['widespread', 'number', ['mr.']],\n",
       " ['widespread', 'voters', ['fourth']],\n",
       " ['widespread', 'size', ['amateur']],\n",
       " ['interest', 'reports', ['course']],\n",
       " ['interest', 'received', ['go']],\n",
       " ['interest', 'jury', ['sherry']],\n",
       " ['interest', 'considering', ['entirely']],\n",
       " ['interest', 'widespread', ['studies']],\n",
       " ['interest', 'election', ['summer']],\n",
       " ['interest', 'number', ['quarters']],\n",
       " ['interest', 'voters', ['dream']],\n",
       " ['interest', 'size', [\"who'd\"]],\n",
       " ['interest', 'city', ['simplify']],\n",
       " ['election', 'received', ['chapter']],\n",
       " ['election', 'jury', ['living']],\n",
       " ['election', 'considering', ['designers']],\n",
       " ['election', 'widespread', ['facilities']],\n",
       " ['election', 'interest', ['right']],\n",
       " ['election', 'number', ['performance']],\n",
       " ['election', 'voters', ['considered']],\n",
       " ['election', 'size', ['shelter']],\n",
       " ['election', 'city', ['obtained']],\n",
       " ['number', 'jury', [\"there's\"]],\n",
       " ['number', 'considering', ['city']],\n",
       " ['number', 'widespread', ['fully']],\n",
       " ['number', 'interest', ['dentist']],\n",
       " ['number', 'election', ['engineer']],\n",
       " ['number', 'voters', ['carbine']],\n",
       " ['number', 'size', ['consists']],\n",
       " ['number', 'city', ['squeezed']],\n",
       " ['voters', 'considering', ['patriot']],\n",
       " ['voters', 'widespread', ['common']],\n",
       " ['voters', 'interest', ['quackery']],\n",
       " ['voters', 'election', ['say']],\n",
       " ['voters', 'number', ['alarmed']],\n",
       " ['voters', 'size', ['effective']],\n",
       " ['voters', 'city', ['smith']],\n",
       " ['size', 'widespread', ['administration']],\n",
       " ['size', 'interest', ['removed']],\n",
       " ['size', 'election', ['conviction']],\n",
       " ['size', 'number', ['hall']],\n",
       " ['size', 'voters', ['power']],\n",
       " ['size', 'city', ['computed']],\n",
       " ['city', 'interest', ['head']],\n",
       " ['city', 'election', ['analysis']],\n",
       " ['city', 'number', ['ever']],\n",
       " ['city', 'voters', ['random']],\n",
       " ['city', 'size', ['still']],\n",
       " ['jury', 'did', ['sort']],\n",
       " ['jury', 'find', [\"'\"]],\n",
       " ['jury', 'many', ['trigger']],\n",
       " ['jury', \"georgia's\", ['maybe']],\n",
       " ['jury', 'registration', ['set']],\n",
       " ['did', 'jury', ['forces']],\n",
       " ['did', 'find', ['nothing']],\n",
       " ['did', 'many', ['smiling']],\n",
       " ['did', \"georgia's\", ['average']],\n",
       " ['did', 'registration', ['fiction']],\n",
       " ['did', 'election', ['journey']],\n",
       " ['find', 'jury', ['early']],\n",
       " ['find', 'did', ['findings']],\n",
       " ['find', 'many', ['defenders']],\n",
       " ['find', \"georgia's\", ['police']],\n",
       " ['find', 'registration', ['through']],\n",
       " ['find', 'election', ['colors']],\n",
       " ['find', 'laws', ['part']],\n",
       " ['many', 'jury', ['utilized']],\n",
       " ['many', 'did', ['burdens']],\n",
       " ['many', 'find', ['planetary']],\n",
       " ['many', \"georgia's\", ['shared']],\n",
       " ['many', 'registration', ['italy']],\n",
       " ['many', 'election', ['balls']],\n",
       " ['many', 'laws', ['car']],\n",
       " ['many', 'inadequate', ['compressed']],\n",
       " [\"georgia's\", 'jury', ['long']],\n",
       " [\"georgia's\", 'did', ['resignation']],\n",
       " [\"georgia's\", 'find', ['sure']],\n",
       " [\"georgia's\", 'many', ['reduce']],\n",
       " [\"georgia's\", 'registration', ['train']],\n",
       " [\"georgia's\", 'election', ['needs']],\n",
       " [\"georgia's\", 'laws', ['devices']],\n",
       " [\"georgia's\", 'inadequate', ['brushed']],\n",
       " [\"georgia's\", 'often', ['service']],\n",
       " ['registration', 'jury', ['paper']],\n",
       " ['registration', 'did', ['institute']],\n",
       " ['registration', 'find', ['ability']],\n",
       " ['registration', 'many', ['beauty']],\n",
       " ['registration', \"georgia's\", ['chicago']],\n",
       " ['registration', 'election', ['assimilation']],\n",
       " ['registration', 'laws', ['amount']],\n",
       " ['registration', 'inadequate', ['fellows']],\n",
       " ['registration', 'often', ['19']],\n",
       " ['registration', 'ambiguous', ['distant']],\n",
       " ['election', 'did', ['feared']],\n",
       " ['election', 'find', ['pioneer']],\n",
       " ['election', 'many', ['equilibrium']],\n",
       " ['election', \"georgia's\", ['current']],\n",
       " ['election', 'registration', ['since']],\n",
       " ['election', 'laws', ['cents']],\n",
       " ['election', 'inadequate', ['ward']],\n",
       " ['election', 'often', ['willingness']],\n",
       " ['election', 'ambiguous', ['money']],\n",
       " ['laws', 'find', ['take']],\n",
       " ['laws', 'many', ['provides']],\n",
       " ['laws', \"georgia's\", ['appealing']],\n",
       " ['laws', 'registration', ['highly']],\n",
       " ['laws', 'election', ['whole']],\n",
       " ['laws', 'inadequate', ['get']],\n",
       " ['laws', 'often', ['getting']],\n",
       " ['laws', 'ambiguous', ['turn']],\n",
       " ['inadequate', 'many', ['broadway']],\n",
       " ['inadequate', \"georgia's\", ['gun']],\n",
       " ['inadequate', 'registration', ['sense']],\n",
       " ['inadequate', 'election', ['empire']],\n",
       " ['inadequate', 'laws', ['lotion']],\n",
       " ['inadequate', 'often', ['solar']],\n",
       " ['inadequate', 'ambiguous', ['yet']],\n",
       " ['often', \"georgia's\", ['testimony']],\n",
       " ['often', 'registration', ['person']],\n",
       " ['often', 'election', ['governor']],\n",
       " ['often', 'laws', ['game']],\n",
       " ['often', 'inadequate', ['running']],\n",
       " ['often', 'ambiguous', ['debate']],\n",
       " ['ambiguous', 'registration', ['each']],\n",
       " ['ambiguous', 'election', ['buzz']],\n",
       " ['ambiguous', 'laws', ['relatively']],\n",
       " ['ambiguous', 'inadequate', ['visit']],\n",
       " ['ambiguous', 'often', ['roles']],\n",
       " ['recommended', 'fulton', ['collective']],\n",
       " ['recommended', 'legislators', ['rate']],\n",
       " ['recommended', 'act', ['conflicting']],\n",
       " ['recommended', 'laws', ['article']],\n",
       " ['recommended', 'studied', ['high']],\n",
       " ['fulton', 'recommended', ['low']],\n",
       " ['fulton', 'legislators', ['upon']],\n",
       " ['fulton', 'act', ['sign']],\n",
       " ['fulton', 'laws', ['sewage']],\n",
       " ['fulton', 'studied', ['immediate']],\n",
       " ['fulton', 'revised', ['thought']],\n",
       " ['legislators', 'recommended', ['survey']],\n",
       " ['legislators', 'fulton', ['open']],\n",
       " ['legislators', 'act', ['feathers']],\n",
       " ['legislators', 'laws', ['problems']],\n",
       " ['legislators', 'studied', ['essex']],\n",
       " ['legislators', 'revised', ['miniature']],\n",
       " ['legislators', 'end', ['stability']],\n",
       " ['act', 'recommended', ['outcome']],\n",
       " ['act', 'fulton', ['recall']],\n",
       " ['act', 'legislators', ['threat']],\n",
       " ['act', 'laws', ['surprised']],\n",
       " ['act', 'studied', ['leaned']],\n",
       " ['act', 'revised', ['extent']],\n",
       " ['act', 'end', ['policy']],\n",
       " ['act', 'improving', ['little']],\n",
       " ['laws', 'recommended', ['decline']],\n",
       " ['laws', 'fulton', ['used']],\n",
       " ['laws', 'legislators', ['respect']],\n",
       " ['laws', 'act', ['8']],\n",
       " ['laws', 'studied', ['words']],\n",
       " ['laws', 'revised', ['wave']],\n",
       " ['laws', 'end', ['hospitality']],\n",
       " ['laws', 'improving', ['essence']],\n",
       " ['studied', 'recommended', ['wide']],\n",
       " ['studied', 'fulton', ['forced']],\n",
       " ['studied', 'legislators', ['drugs']],\n",
       " ['studied', 'act', ['essentially']],\n",
       " ['studied', 'laws', ['uncle']],\n",
       " ['studied', 'revised', ['bats']],\n",
       " ['studied', 'end', ['planning']],\n",
       " ['studied', 'improving', ['moonlight']],\n",
       " ['revised', 'fulton', ['designed']],\n",
       " ['revised', 'legislators', ['friday']],\n",
       " ['revised', 'act', ['yards']],\n",
       " ['revised', 'laws', ['b']],\n",
       " ['revised', 'studied', ['employees']],\n",
       " ['revised', 'end', ['series']],\n",
       " ['revised', 'improving', ['years']],\n",
       " ['end', 'legislators', ['structure']],\n",
       " ['end', 'act', ['hank']],\n",
       " ['end', 'laws', ['aristotle']],\n",
       " ['end', 'studied', ['till']],\n",
       " ['end', 'revised', ['aid']],\n",
       " ['end', 'improving', ['ever']],\n",
       " ['improving', 'act', ['analyzed']],\n",
       " ['improving', 'laws', ['thought']],\n",
       " ['improving', 'studied', ['alive']],\n",
       " ['improving', 'revised', ['calendars']],\n",
       " ['improving', 'end', ['committee']],\n",
       " ['grand', 'jury', ['spade']],\n",
       " ['grand', 'commented', ['realize']],\n",
       " ['grand', 'number', ['possibility']],\n",
       " ['grand', 'topics', ['being']],\n",
       " ['grand', 'among', ['attention']],\n",
       " ['jury', 'grand', ['c']],\n",
       " ['jury', 'commented', ['sequence']],\n",
       " ['jury', 'number', ['mistakes']],\n",
       " ['jury', 'topics', ['best']],\n",
       " ['jury', 'among', ['translate']],\n",
       " ['jury', 'atlanta', ['between']],\n",
       " ['commented', 'grand', ['wilson']],\n",
       " ['commented', 'jury', ['attach']],\n",
       " ['commented', 'number', ['measures']],\n",
       " ['commented', 'topics', ['ontological']],\n",
       " ['commented', 'among', ['status']],\n",
       " ['commented', 'atlanta', ['tend']],\n",
       " ['commented', 'fulton', ['really']],\n",
       " ['number', 'grand', ['city']],\n",
       " ['number', 'jury', ['antique']],\n",
       " ['number', 'commented', ['bomb']],\n",
       " ['number', 'topics', ['considering']],\n",
       " ['number', 'among', ['anyone']],\n",
       " ['number', 'atlanta', ['becoming']],\n",
       " ['number', 'fulton', ['u.']],\n",
       " ['number', 'county', ['assertion']],\n",
       " ['topics', 'grand', ['write']],\n",
       " ['topics', 'jury', ['twisted']],\n",
       " ['topics', 'commented', ['restrictions']],\n",
       " ['topics', 'number', ['chairmen']],\n",
       " ['topics', 'among', ['vote']],\n",
       " ['topics', 'atlanta', ['modern']],\n",
       " ['topics', 'fulton', ['ekstrohm']],\n",
       " ['topics', 'county', ['gives']],\n",
       " ['topics', 'purchasing', ['nothing']],\n",
       " ['among', 'grand', ['gorton']],\n",
       " ['among', 'jury', ['c']],\n",
       " ['among', 'commented', ['value']],\n",
       " ['among', 'number', ['silly']],\n",
       " ['among', 'topics', ['textile']],\n",
       " ['among', 'atlanta', ['steering']],\n",
       " ['among', 'fulton', ['found']],\n",
       " ['among', 'county', ['medical']],\n",
       " ['among', 'purchasing', ['felt']],\n",
       " ['among', 'departments', ['tony']],\n",
       " ['atlanta', 'jury', ['station']],\n",
       " ['atlanta', 'commented', ['urethane']],\n",
       " ['atlanta', 'number', ['appearing']],\n",
       " ['atlanta', 'topics', ['compliance']],\n",
       " ['atlanta', 'among', ['east-west']],\n",
       " ['atlanta', 'fulton', ['representation']],\n",
       " ['atlanta', 'county', ['world']],\n",
       " ['atlanta', 'purchasing', ['tradition']],\n",
       " ['atlanta', 'departments', ['drink']],\n",
       " ['atlanta', 'well', ['owed']],\n",
       " ['fulton', 'commented', ['captain']],\n",
       " ['fulton', 'number', ['synthesis']],\n",
       " ['fulton', 'topics', ['mile']],\n",
       " ['fulton', 'among', ['years']],\n",
       " ['fulton', 'atlanta', ['thought']],\n",
       " ['fulton', 'county', ['suitcase']],\n",
       " ['fulton', 'purchasing', ['tall']],\n",
       " ['fulton', 'departments', ['replacement']],\n",
       " ['fulton', 'well', ['claimant']],\n",
       " ['fulton', 'operated', ['perform']],\n",
       " ['county', 'number', ['troubles']],\n",
       " ['county', 'topics', ['assigning']],\n",
       " ['county', 'among', ['general']],\n",
       " ['county', 'atlanta', ['boating']],\n",
       " ['county', 'fulton', ['consideration']],\n",
       " ['county', 'purchasing', ['soon']],\n",
       " ['county', 'departments', ['seaman']],\n",
       " ['county', 'well', ['class']],\n",
       " ['county', 'operated', ['january']],\n",
       " ['county', 'follow', ['proof']],\n",
       " ['purchasing', 'topics', ['recognized']],\n",
       " ['purchasing', 'among', ['bacterial']],\n",
       " ['purchasing', 'atlanta', ['manner']],\n",
       " ['purchasing', 'fulton', ['government']],\n",
       " ['purchasing', 'county', ['ninety']],\n",
       " ['purchasing', 'departments', ['twenty-four']],\n",
       " ['purchasing', 'well', ['spoiled']],\n",
       " ['purchasing', 'operated', ['determined']],\n",
       " ['purchasing', 'follow', ['games']],\n",
       " ['purchasing', 'generally', ['nose']],\n",
       " ['departments', 'among', ['better']],\n",
       " ['departments', 'atlanta', ['construction']],\n",
       " ['departments', 'fulton', ['domestic']],\n",
       " ['departments', 'county', ['stumbling']],\n",
       " ['departments', 'purchasing', ['three']],\n",
       " ['departments', 'well', ['eager']],\n",
       " ['departments', 'operated', ['nod']],\n",
       " ['departments', 'follow', ['research']],\n",
       " ['departments', 'generally', ['solemn']],\n",
       " ['departments', 'accepted', ['song']],\n",
       " ['well', 'atlanta', ['exchange']],\n",
       " ['well', 'fulton', ['strasbourg']],\n",
       " ['well', 'county', ['mentioned']],\n",
       " ['well', 'purchasing', ['fans']],\n",
       " ['well', 'departments', ['husband']],\n",
       " ['well', 'operated', ['seems']],\n",
       " ['well', 'follow', ['separate']],\n",
       " ['well', 'generally', ['doing']],\n",
       " ['well', 'accepted', ['controversy']],\n",
       " ['well', 'practices', ['chinese']],\n",
       " ['operated', 'fulton', ['being']],\n",
       " ['operated', 'county', ['n']],\n",
       " ['operated', 'purchasing', ['material']],\n",
       " ['operated', 'departments', ['carleton']],\n",
       " ['operated', 'well', ['coldly']],\n",
       " ['operated', 'follow', ['lectures']],\n",
       " ['operated', 'generally', ['anatomy']],\n",
       " ['operated', 'accepted', ['wages']],\n",
       " ['operated', 'practices', ['queer']],\n",
       " ['operated', 'best', ['improved']],\n",
       " ['follow', 'county', ['series']],\n",
       " ['follow', 'purchasing', ['member']],\n",
       " ['follow', 'departments', ['test']],\n",
       " ['follow', 'well', ['mullins']],\n",
       " ['follow', 'operated', ['bet']],\n",
       " ['follow', 'generally', ['along']],\n",
       " ['follow', 'accepted', ['against']],\n",
       " ['follow', 'practices', ['record']],\n",
       " ['follow', 'best', ['sums']],\n",
       " ['follow', 'interest', ['yes']],\n",
       " ['generally', 'purchasing', ['coarse']],\n",
       " ['generally', 'departments', ['operating']],\n",
       " ['generally', 'well', ['every']],\n",
       " ['generally', 'operated', ['gathering']],\n",
       " ['generally', 'follow', ['motion']],\n",
       " ['generally', 'accepted', ['addition']],\n",
       " ['generally', 'practices', ['investigation']],\n",
       " ['generally', 'best', ['decade']],\n",
       " ['generally', 'interest', ['very']],\n",
       " ['generally', 'both', ['surrounded']],\n",
       " ['accepted', 'departments', ['sign']],\n",
       " ['accepted', 'well', ['death']],\n",
       " ['accepted', 'operated', ['yourself']],\n",
       " ['accepted', 'follow', ['safe']],\n",
       " ['accepted', 'generally', ['turn']],\n",
       " ['accepted', 'practices', ['killer']],\n",
       " ['accepted', 'best', ['fought']],\n",
       " ['accepted', 'interest', ['indices']],\n",
       " ['accepted', 'both', ['sigh']],\n",
       " ['accepted', 'governments', ['emphasis']],\n",
       " ['practices', 'well', ['weider']],\n",
       " ['practices', 'operated', ['policies']],\n",
       " ['practices', 'follow', ['spine']],\n",
       " ['practices', 'generally', ['plays']],\n",
       " ['practices', 'accepted', ['adequate']],\n",
       " ['practices', 'best', ['hour']],\n",
       " ['practices', 'interest', ['project']],\n",
       " ['practices', 'both', ['fabrication']],\n",
       " ['practices', 'governments', ['account']],\n",
       " ['best', 'operated', ['back']],\n",
       " ['best', 'follow', ['helper']],\n",
       " ['best', 'generally', ['groups']],\n",
       " ['best', 'accepted', ['sixth']],\n",
       " ['best', 'practices', ['faulty']],\n",
       " ['best', 'interest', ['thing']],\n",
       " ['best', 'both', ['old']],\n",
       " ['best', 'governments', ['should']],\n",
       " ['interest', 'follow', ['available']],\n",
       " ['interest', 'generally', ['disclosed']],\n",
       " ['interest', 'accepted', ['structural']],\n",
       " ['interest', 'practices', ['scientists']],\n",
       " ['interest', 'best', ['separate']],\n",
       " ['interest', 'both', ['trips']],\n",
       " ['interest', 'governments', ['felt']],\n",
       " ['both', 'generally', ['control']],\n",
       " ['both', 'accepted', ['respond']],\n",
       " ['both', 'practices', ['purdew']],\n",
       " ['both', 'best', ['should']],\n",
       " ['both', 'interest', ['type']],\n",
       " ['both', 'governments', ['affirm']],\n",
       " ['governments', 'accepted', ['security']],\n",
       " ['governments', 'practices', ['say']],\n",
       " ['governments', 'best', ['ability']],\n",
       " ['governments', 'interest', ['whose']],\n",
       " ['governments', 'both', ['excess']],\n",
       " ['merger', 'proposed', ['faulty']],\n",
       " ['proposed', 'merger', ['el']],\n",
       " ['however', 'jury', ['compulsion']],\n",
       " ['however', 'believes', ['curt']],\n",
       " ['however', 'offices', ['potato']],\n",
       " ['however', 'should', ['trees']],\n",
       " ['however', 'combined', ['body']],\n",
       " ['jury', 'however', ['engineering']],\n",
       " ['jury', 'believes', ['post']],\n",
       " ['jury', 'offices', ['around']],\n",
       " ['jury', 'should', ['association']],\n",
       " ['jury', 'combined', ['chewing']],\n",
       " ['jury', 'achieve', ['further']],\n",
       " ['believes', 'however', ['hate']],\n",
       " ['believes', 'jury', ['comes']],\n",
       " ['believes', 'offices', ['abolition']],\n",
       " ['believes', 'should', ['productivity']],\n",
       " ['believes', 'combined', ['personalities']],\n",
       " ['believes', 'achieve', ['kills']],\n",
       " ['believes', 'greater', ['existed']],\n",
       " ['offices', 'however', ['buck']],\n",
       " ['offices', 'jury', ['movement']],\n",
       " ['offices', 'believes', ['day']],\n",
       " ['offices', 'should', ['linda']],\n",
       " ['offices', 'combined', ['motel']],\n",
       " ['offices', 'achieve', ['nothing']],\n",
       " ['offices', 'greater', ['line']],\n",
       " ['offices', 'efficiency', ['entering']],\n",
       " ['should', 'however', ['utah']],\n",
       " ['should', 'jury', ['quivering']],\n",
       " ['should', 'believes', ['fell']],\n",
       " ['should', 'offices', ['wheat']],\n",
       " ['should', 'combined', ['pleased']],\n",
       " ['should', 'achieve', ['connected']],\n",
       " ['should', 'greater', ['panic']],\n",
       " ['should', 'efficiency', ['narrator']],\n",
       " ['should', 'reduce', ['want']],\n",
       " ['combined', 'however', ['reaches']],\n",
       " ['combined', 'jury', ['three']],\n",
       " ['combined', 'believes', ['carl']],\n",
       " ['combined', 'offices', ['just']],\n",
       " ['combined', 'should', ['agent']],\n",
       " ['combined', 'achieve', ['body']],\n",
       " ['combined', 'greater', ['go']],\n",
       " ['combined', 'efficiency', ['negative']],\n",
       " ['combined', 'reduce', ['holy']],\n",
       " ['combined', 'cost', ['af']],\n",
       " ['achieve', 'jury', ['female']],\n",
       " ['achieve', 'believes', ['near']],\n",
       " ['achieve', 'offices', ['identify']],\n",
       " ['achieve', 'should', ['armies']],\n",
       " ['achieve', 'combined', ['see']],\n",
       " ['achieve', 'greater', ['predicting']],\n",
       " ['achieve', 'efficiency', ['drift']],\n",
       " ['achieve', 'reduce', ['black']],\n",
       " ['achieve', 'cost', ['make']],\n",
       " ['achieve', 'administration', ['embodied']],\n",
       " ['greater', 'believes', ['boredom']],\n",
       " ['greater', 'offices', ['differences']],\n",
       " ['greater', 'should', ['water']],\n",
       " ['greater', 'combined', ['verbs']],\n",
       " ['greater', 'achieve', ['cloth']],\n",
       " ['greater', 'efficiency', ['owner']],\n",
       " ['greater', 'reduce', ['baseball']],\n",
       " ['greater', 'cost', ['afternoon']],\n",
       " ['greater', 'administration', ['last']],\n",
       " ['efficiency', 'offices', ['twist']],\n",
       " ['efficiency', 'should', ['primarily']],\n",
       " ['efficiency', 'combined', ['sponsors']],\n",
       " ['efficiency', 'achieve', ['magnified']],\n",
       " ['efficiency', 'greater', ['context']],\n",
       " ['efficiency', 'reduce', ['little']],\n",
       " ['efficiency', 'cost', ['korea']],\n",
       " ['efficiency', 'administration', ['very']],\n",
       " ['reduce', 'should', ['predecessors']],\n",
       " ['reduce', 'combined', ['reactions']],\n",
       " ['reduce', 'achieve', ['advertised']],\n",
       " ['reduce', 'greater', ['large']],\n",
       " ['reduce', 'efficiency', ['seed']],\n",
       " ['reduce', 'cost', ['persons']],\n",
       " ['reduce', 'administration', ['raged']],\n",
       " ['cost', 'combined', ['terrace']],\n",
       " ['cost', 'achieve', ['penetrating']],\n",
       " ['cost', 'greater', ['selection']],\n",
       " ['cost', 'efficiency', ['labor']],\n",
       " ['cost', 'reduce', ['mean']],\n",
       " ['cost', 'administration', ['attitude']],\n",
       " ['administration', 'achieve', ['own']],\n",
       " ['administration', 'greater', ['stephen']],\n",
       " ['administration', 'efficiency', ['jewish']],\n",
       " ['administration', 'reduce', ['adolescents']],\n",
       " ['administration', 'cost', ['abundant']],\n",
       " ['city', 'purchasing', ['areas']],\n",
       " ['city', 'department', ['while']],\n",
       " ['city', 'jury', ['equate']],\n",
       " ['city', 'lacking', ['corridor']],\n",
       " ['city', 'experienced', ['go']],\n",
       " ['purchasing', 'city', ['lord']],\n",
       " ['purchasing', 'department', ['throat']],\n",
       " ['purchasing', 'jury', ['categories']],\n",
       " ['purchasing', 'lacking', ['water']],\n",
       " ['purchasing', 'experienced', ['rotating']],\n",
       " ['purchasing', 'clerical', ['faith']],\n",
       " ['department', 'city', ['providing']],\n",
       " ['department', 'purchasing', [\"who's\"]],\n",
       " ['department', 'jury', ['central']],\n",
       " ['department', 'lacking', ['quickly']],\n",
       " ['department', 'experienced', ['enters']],\n",
       " ['department', 'clerical', ['saw']],\n",
       " ['department', 'personnel', ['strength']],\n",
       " ['jury', 'city', ['42']],\n",
       " ['jury', 'purchasing', ['hetman']],\n",
       " ['jury', 'department', ['noticed']],\n",
       " ['jury', 'lacking', ['liberals']],\n",
       " ['jury', 'experienced', ['drums']],\n",
       " ['jury', 'clerical', ['odds']],\n",
       " ['jury', 'personnel', ['culture']],\n",
       " ['jury', 'result', ['motionless']],\n",
       " ['lacking', 'city', ['policy']],\n",
       " ['lacking', 'purchasing', ['fighting']],\n",
       " ['lacking', 'department', ['particular']],\n",
       " ['lacking', 'jury', ['glance']],\n",
       " ['lacking', 'experienced', ['behind']],\n",
       " ['lacking', 'clerical', ['congress']],\n",
       " ['lacking', 'personnel', ['glory']],\n",
       " ['lacking', 'result', ['knew']],\n",
       " ['lacking', 'city', ['human']],\n",
       " ['experienced', 'city', ['proposed']],\n",
       " ['experienced', 'purchasing', ['men']],\n",
       " ['experienced', 'department', ['off']],\n",
       " ['experienced', 'jury', ['deductions']],\n",
       " ['experienced', 'lacking', ['seeks']],\n",
       " ['experienced', 'clerical', ['classic']],\n",
       " ['experienced', 'personnel', ['debris']],\n",
       " ['experienced', 'result', ['scratching']],\n",
       " ['experienced', 'city', ['friends']],\n",
       " ['experienced', 'personnel', ['nbc']],\n",
       " ['clerical', 'purchasing', ['touched']],\n",
       " ['clerical', 'department', ['aspect']],\n",
       " ['clerical', 'jury', ['found']],\n",
       " ['clerical', 'lacking', ['did']],\n",
       " ['clerical', 'experienced', ['factors']],\n",
       " ['clerical', 'personnel', ['column']],\n",
       " ['clerical', 'result', ['medicine']],\n",
       " ['clerical', 'city', ['resisted']],\n",
       " ['clerical', 'personnel', ['men']],\n",
       " ['clerical', 'policies', ['paint']],\n",
       " ['personnel', 'department', ['significance']],\n",
       " ['personnel', 'jury', ['suicide']],\n",
       " ['personnel', 'lacking', ['u.s.']],\n",
       " ['personnel', 'experienced', ['king']],\n",
       " ['personnel', 'clerical', ['movies']],\n",
       " ['personnel', 'result', ['bankruptcy']],\n",
       " ['personnel', 'city', ['deeper']],\n",
       " ['personnel', 'policies', ['charges']],\n",
       " ['result', 'jury', ['after']],\n",
       " ['result', 'lacking', ['put']],\n",
       " ['result', 'experienced', ['opportunity']],\n",
       " ['result', 'clerical', ['front']],\n",
       " ['result', 'personnel', ['bits']],\n",
       " ['result', 'city', ['drift']],\n",
       " ['result', 'personnel', ['completed']],\n",
       " ['result', 'policies', ['once']],\n",
       " ['city', 'lacking', ['noted']],\n",
       " ['city', 'experienced', ['individuals']],\n",
       " ['city', 'clerical', ['jess']],\n",
       " ['city', 'personnel', ['serious']],\n",
       " ['city', 'result', [\"you've\"]],\n",
       " ['city', 'personnel', ['train']],\n",
       " ['city', 'policies', ['sides']],\n",
       " ['personnel', 'experienced', ['readings']],\n",
       " ['personnel', 'clerical', ['component']],\n",
       " ['personnel', 'result', ['handicap']],\n",
       " ['personnel', 'city', ['tangent']],\n",
       " ['personnel', 'policies', ['must']],\n",
       " ['policies', 'clerical', ['preventing']],\n",
       " ['policies', 'personnel', ['upon']],\n",
       " ['policies', 'result', ['gulf']],\n",
       " ['policies', 'city', ['jew']],\n",
       " ['policies', 'personnel', ['story']],\n",
       " ['urged', 'city', ['throwing']],\n",
       " ['urged', 'take', ['every']],\n",
       " ['urged', 'steps', ['law']],\n",
       " ['urged', 'remedy', ['oriental']],\n",
       " ['urged', 'problem', ['leagues']],\n",
       " ['city', 'urged', ['discrete']],\n",
       " ['city', 'take', ['restless']],\n",
       " ['city', 'steps', ['crowd']],\n",
       " ['city', 'remedy', ['coffee']],\n",
       " ['city', 'problem', ['lend']],\n",
       " ['take', 'urged', ['dairy']],\n",
       " ['take', 'city', ['technology']],\n",
       " ['take', 'steps', ['interruption']],\n",
       " ['take', 'remedy', ['letter']],\n",
       " ['take', 'problem', ['moved']],\n",
       " ['steps', 'urged', ['pels']],\n",
       " ['steps', 'city', ['far']],\n",
       " ['steps', 'take', ['impossible']],\n",
       " ['steps', 'remedy', ['dilution']],\n",
       " ['steps', 'problem', ['themselves']],\n",
       " ['remedy', 'urged', ['police']],\n",
       " ['remedy', 'city', ['mr.']],\n",
       " ['remedy', 'take', ['bottles']],\n",
       " ['remedy', 'steps', ['plates']],\n",
       " ['remedy', 'problem', ['pencil']],\n",
       " ['problem', 'urged', ['used']],\n",
       " ['problem', 'city', ['old']],\n",
       " ['problem', 'take', ['measure']],\n",
       " ['problem', 'steps', ['thrown']],\n",
       " ['problem', 'remedy', ['n']],\n",
       " ['implementation', \"georgia's\", ['grown']],\n",
       " ['implementation', 'automobile', ['sketch']],\n",
       " ['implementation', 'title', ['concord']],\n",
       " ['implementation', 'law', ['coming']],\n",
       " ['implementation', 'also', ['seat']],\n",
       " [\"georgia's\", 'implementation', ['many']],\n",
       " [\"georgia's\", 'automobile', ['reading']],\n",
       " [\"georgia's\", 'title', ['upward']],\n",
       " [\"georgia's\", 'law', ['caused']],\n",
       " [\"georgia's\", 'also', ['boredom']],\n",
       " ...]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print training set\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now build the training set for the brown dataset which will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs, word_to_index, index_to_word = build_indices(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "uh8TDnKxwaml",
    "outputId": "a2fe935e-117d-441d-f8cc-d8a3f548b2ae"
   },
   "outputs": [],
   "source": [
    "training_set, words_list = build_training_set(brown.sents(),word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "yJD0RYekHXBD",
    "outputId": "3858e3b7-05bd-4c84-e11a-08a6deac335c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fulton', 'county', ['rigidly']],\n",
       " ['fulton', 'grand', ['dr.']],\n",
       " ['fulton', 'jury', ['son']],\n",
       " ['fulton', 'friday', ['thing']],\n",
       " ['fulton', 'investigation', ['oils']],\n",
       " ['county', 'fulton', ['sorbed']],\n",
       " ['county', 'grand', ['corps']],\n",
       " ['county', 'jury', ['henrietta']],\n",
       " ['county', 'friday', ['pouring']],\n",
       " ['county', 'investigation', ['without']]]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 10 examples in the trainigng set\n",
    "training_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "28etYfROwamx",
    "outputId": "76579b98-d211-4af2-86b5-4916a188e536"
   },
   "outputs": [],
   "source": [
    "def build_model(training_set, initial_alpha = 0.025, min_alpha = 0.0001, n_iters = 5, my_lambda = 0, vector_size = 30):\n",
    "    word_vectors = {}\n",
    "    \n",
    "    # initialize word vectors\n",
    "    for n in range(len(words_list)):\n",
    "        word_vectors[words_list[n]] = np.random.rand(vector_size,1) - 0.5\n",
    "    \n",
    "\n",
    "    alpha = initial_alpha\n",
    "    for t in range(n_iters):\n",
    "        training_set = shuffle(training_set)\n",
    "        objective = 0\n",
    "        for ex in training_set:\n",
    "            w = ex[0]\n",
    "            w_p = ex[1]\n",
    "            w_n_list = ex[2]\n",
    "            w_v = word_vectors[w]\n",
    "            w_p_v = word_vectors[w_p]\n",
    "            word_vectors[w_p] = w_p_v + alpha*(((1-sigmoid(np.dot(w_v.T,w_p_v)))*w_v)-my_lambda*w_p_v)\n",
    "            objective += np.log((sigmoid(np.dot(w_v.T,w_p_v))))\n",
    "\n",
    "            for n in range(len(w_n_list)):\n",
    "                w_n = w_n_list[n]\n",
    "                w_n_v = word_vectors[w_n]\n",
    "                word_vectors[w_n] = w_n_v + alpha*((-(1-sigmoid(-np.dot(w_v.T,w_n_v)))*w_v)-my_lambda*w_n_v)      \n",
    "                objective += np.log((sigmoid(-np.dot(w_v.T,w_p_v))))\n",
    "     \n",
    "        alpha = initial_alpha - ((initial_alpha - min_alpha) * t / n_iters)\n",
    "        print(\"alpha: \",alpha)\n",
    "        print(\"Iteration: \", t)\n",
    "        print(\"Objective: \", objective)\n",
    "        print(\"cosine of words 'friend' and 'fellow': \",np.dot(word_vectors['friend'].T, word_vectors['fellow']))\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  0.025\n",
      "Iteration:  0\n",
      "Objective:  [[-4630731.69185037]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.10466798]]\n",
      "alpha:  0.020020000000000003\n",
      "Iteration:  1\n",
      "Objective:  [[-4765999.2210608]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.43478501]]\n",
      "alpha:  0.015040000000000001\n",
      "Iteration:  2\n",
      "Objective:  [[-4904047.0657708]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.66246879]]\n",
      "alpha:  0.010060000000000001\n",
      "Iteration:  3\n",
      "Objective:  [[-4987769.69572253]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.90364292]]\n",
      "alpha:  0.005080000000000001\n",
      "Iteration:  4\n",
      "Objective:  [[-5038087.76193104]]\n",
      "cosine of words 'friend' and 'fellow':  [[0.96071628]]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = build_model(training_set, initial_alpha = 0.025, min_alpha = 0.0001, n_iters = 5, my_lambda = 0, vector_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4a8La9LNwamz"
   },
   "outputs": [],
   "source": [
    "def most_similar(word, word_vectors):\n",
    "    pq = PriorityQueue()\n",
    "    for w in word_vectors.keys():\n",
    "        pq.put((-np.dot(word_vectors[word].T, word_vectors[w]), w))\n",
    "    return pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi0EKiQwwam1"
   },
   "outputs": [],
   "source": [
    "pq = most_similar('book', word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "7ndrjm5wwam2",
    "outputId": "9ae23791-b143-4ca2-a15d-bb908ff0d374",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-3.03128716]]), 'book')\n",
      "(array([[-2.46231942]]), 'hardy')\n",
      "(array([[-2.43381448]]), 'poems')\n",
      "(array([[-2.31666559]]), 'dr.')\n",
      "(array([[-2.25809943]]), 'fiction')\n",
      "(array([[-2.2277872]]), 'written')\n",
      "(array([[-2.18837499]]), 'samuel')\n",
      "(array([[-2.04982676]]), 'jr.')\n",
      "(array([[-2.03779327]]), 'jazz')\n",
      "(array([[-1.97606912]]), 'writing')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(pq.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "meIMEyIPwam4",
    "outputId": "9987faa7-42ca-4dad-cab3-290edf8a9a77",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-3.42139827]]), 'eight')\n",
      "(array([[-2.88453067]]), 'approximately')\n",
      "(array([[-2.47945924]]), 'per')\n",
      "(array([[-2.38159085]]), 'hundred')\n",
      "(array([[-2.36105726]]), '30')\n",
      "(array([[-2.34564456]]), 'million')\n",
      "(array([[-2.31762669]]), 'ten')\n",
      "(array([[-2.2468595]]), '25')\n",
      "(array([[-2.22857262]]), '100')\n",
      "(array([[-2.19047865]]), 'june')\n"
     ]
    }
   ],
   "source": [
    "pq = most_similar('eight', word_vectors)\n",
    "for i in range(10):\n",
    "    print(pq.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some questions\n",
    "- Even though \"opinion\" and \"story\" does not appear in the context of \"book\" (window size 5) still word2vec puts these close to book, which is remarkable. Can you find this by building a co-occurrence matrix?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
